sls-sm-6 1
SLURM_JOBID=81841
SLURM_TASKID=3
[32m[0321 09:28:32 @logger.py:74][0m Argv: drf_run.py --model_name=fcn2 --bitw=8 --bita=32 --quant_ends=True
[32m[0321 09:29:06 @parallel.py:282][0m [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[32m[0321 09:29:06 @drf_run.py:140][0m Using 6 threads
('whole utterance size', 75290)
[32m[0321 09:29:07 @drf_run.py:165][0m 18822 utterances per val epoch
[32m[0321 09:29:07 @drf_run.py:166][0m Using host: sls-sm-6
[32m[0321 09:29:07 @inference_runner.py:80][0m InferenceRunner will eval 18822 iterations
[32m[0321 09:29:07 @__init__.py:20][0m [5m[31mWRN[0m get_nr_gpu will not be automatically imported any more! Please do `from tensorpack.utils.gpu import get_nr_gpu`
[32m[0321 09:29:07 @drf_run.py:188][0m Using GPU: 1
[32m[0321 09:29:07 @interface.py:34][0m Automatically applying QueueInput on the DataFlow.
[32m[0321 09:29:07 @input_source.py:193][0m Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
[32m[0321 09:29:07 @training.py:108][0m Building graph for training tower 0 ...
[32m[0321 09:29:07 @registry.py:122][0m linear0 input: [None, 1000]
[32m[0321 09:29:07 @drf_run.py:70][0m Quantizing weight linear0/W
[32m[0321 09:29:07 @drf_run.py:70][0m Quantizing weight linear0/b
[32m[0321 09:29:07 @drf_run.py:58][0m Not quantizing linear0/bn/beta
[32m[0321 09:29:07 @drf_run.py:58][0m Not quantizing linear0/bn/gamma
[32m[0321 09:29:07 @drf_run.py:58][0m Not quantizing linear0/bn/mean/EMA
[32m[0321 09:29:07 @drf_run.py:58][0m Not quantizing linear0/bn/variance/EMA
[32m[0321 09:29:07 @registry.py:130][0m linear0 output: [None, 504]
[32m[0321 09:29:07 @registry.py:122][0m linear1 input: [None, 504]
[32m[0321 09:29:07 @drf_run.py:70][0m Quantizing weight linear1/W
[32m[0321 09:29:07 @drf_run.py:70][0m Quantizing weight linear1/b
[32m[0321 09:29:07 @drf_run.py:58][0m Not quantizing linear1/bn/beta
[32m[0321 09:29:07 @drf_run.py:58][0m Not quantizing linear1/bn/gamma
[32m[0321 09:29:07 @drf_run.py:58][0m Not quantizing linear1/bn/mean/EMA
[32m[0321 09:29:07 @drf_run.py:58][0m Not quantizing linear1/bn/variance/EMA
[32m[0321 09:29:07 @registry.py:130][0m linear1 output: [None, 504]
[32m[0321 09:29:07 @registry.py:122][0m linear2 input: [None, 504]
[32m[0321 09:29:07 @drf_run.py:70][0m Quantizing weight linear2/W
[32m[0321 09:29:07 @drf_run.py:70][0m Quantizing weight linear2/b
[32m[0321 09:29:07 @drf_run.py:58][0m Not quantizing linear2/bn/beta
[32m[0321 09:29:07 @drf_run.py:58][0m Not quantizing linear2/bn/gamma
[32m[0321 09:29:07 @drf_run.py:58][0m Not quantizing linear2/bn/mean/EMA
[32m[0321 09:29:07 @drf_run.py:58][0m Not quantizing linear2/bn/variance/EMA
[32m[0321 09:29:07 @registry.py:130][0m linear2 output: [None, 504]
[32m[0321 09:29:07 @registry.py:122][0m linear3 input: [None, 504]
[32m[0321 09:29:07 @drf_run.py:70][0m Quantizing weight linear3/W
[32m[0321 09:29:07 @drf_run.py:70][0m Quantizing weight linear3/b
[32m[0321 09:29:07 @drf_run.py:58][0m Not quantizing linear3/bn/beta
[32m[0321 09:29:07 @drf_run.py:58][0m Not quantizing linear3/bn/gamma
[32m[0321 09:29:07 @drf_run.py:58][0m Not quantizing linear3/bn/mean/EMA
[32m[0321 09:29:07 @drf_run.py:58][0m Not quantizing linear3/bn/variance/EMA
[32m[0321 09:29:07 @registry.py:130][0m linear3 output: [None, 504]
[32m[0321 09:29:07 @registry.py:122][0m last_linear input: [None, 504]
[32m[0321 09:29:07 @drf_run.py:70][0m Quantizing weight last_linear/W
[32m[0321 09:29:07 @drf_run.py:70][0m Quantizing weight last_linear/b
[32m[0321 09:29:07 @registry.py:130][0m last_linear output: [None, 255]
[32m[0321 09:29:07 @drf_run.py:112][0m Adding activation tensors to summary: [<tf.Tensor 'tower0/last_linear/output:0' shape=(?, 255) dtype=float32>, <tf.Tensor 'tower0/output:0' shape=(?, 255) dtype=float32>]
[32m[0321 09:29:07 @regularize.py:81][0m regularize_cost() found 5 tensors.
[32m[0321 09:29:07 @regularize.py:18][0m Applying regularizer for linear0/W:0, linear1/W:0, linear2/W:0, linear3/W:0, last_linear/W:0
[32m[0321 09:29:07 @drf_run.py:123][0m Parameter count: {'mults': 1398600, 'weights': 1398855}
[32m[0321 09:29:08 @model_utils.py:49][0m [36mModel Parameters: 
[0mname                shape           dim
------------------  -----------  ------
linear0/W:0         [1000, 504]  504000
linear0/b:0         [504]           504
linear0/bn/beta:0   [504]           504
linear0/bn/gamma:0  [504]           504
linear1/W:0         [504, 504]   254016
linear1/b:0         [504]           504
linear1/bn/beta:0   [504]           504
linear1/bn/gamma:0  [504]           504
linear2/W:0         [504, 504]   254016
linear2/b:0         [504]           504
linear2/bn/beta:0   [504]           504
linear2/bn/gamma:0  [504]           504
linear3/W:0         [504, 504]   254016
linear3/b:0         [504]           504
linear3/bn/beta:0   [504]           504
linear3/bn/gamma:0  [504]           504
last_linear/W:0     [504, 255]   128520
last_linear/b:0     [255]           255[36m
Total #vars=18, #params=1400871, size=5.34MB[0m
[32m[0321 09:29:08 @base.py:196][0m Setup callbacks graph ...
[32m[0321 09:29:08 @predict.py:42][0m Building predictor tower 'InferenceTower' on device /gpu:0 ...
[32m[0321 09:29:08 @drf_run.py:70][0m Quantizing weight linear0/W
[32m[0321 09:29:08 @drf_run.py:70][0m Quantizing weight linear0/b
[32m[0321 09:29:08 @drf_run.py:58][0m Not quantizing linear0/bn/beta
[32m[0321 09:29:08 @drf_run.py:58][0m Not quantizing linear0/bn/gamma
[32m[0321 09:29:08 @drf_run.py:58][0m Not quantizing linear0/bn/mean/EMA
[32m[0321 09:29:08 @drf_run.py:58][0m Not quantizing linear0/bn/variance/EMA
[32m[0321 09:29:08 @drf_run.py:70][0m Quantizing weight linear1/W
[32m[0321 09:29:09 @drf_run.py:70][0m Quantizing weight linear1/b
[32m[0321 09:29:09 @drf_run.py:58][0m Not quantizing linear1/bn/beta
[32m[0321 09:29:09 @drf_run.py:58][0m Not quantizing linear1/bn/gamma
[32m[0321 09:29:09 @drf_run.py:58][0m Not quantizing linear1/bn/mean/EMA
[32m[0321 09:29:09 @drf_run.py:58][0m Not quantizing linear1/bn/variance/EMA
[32m[0321 09:29:09 @drf_run.py:70][0m Quantizing weight linear2/W
[32m[0321 09:29:09 @drf_run.py:70][0m Quantizing weight linear2/b
[32m[0321 09:29:09 @drf_run.py:58][0m Not quantizing linear2/bn/beta
[32m[0321 09:29:09 @drf_run.py:58][0m Not quantizing linear2/bn/gamma
[32m[0321 09:29:09 @drf_run.py:58][0m Not quantizing linear2/bn/mean/EMA
[32m[0321 09:29:09 @drf_run.py:58][0m Not quantizing linear2/bn/variance/EMA
[32m[0321 09:29:09 @drf_run.py:70][0m Quantizing weight linear3/W
[32m[0321 09:29:09 @drf_run.py:70][0m Quantizing weight linear3/b
[32m[0321 09:29:09 @drf_run.py:58][0m Not quantizing linear3/bn/beta
[32m[0321 09:29:09 @drf_run.py:58][0m Not quantizing linear3/bn/gamma
[32m[0321 09:29:09 @drf_run.py:58][0m Not quantizing linear3/bn/mean/EMA
[32m[0321 09:29:09 @drf_run.py:58][0m Not quantizing linear3/bn/variance/EMA
[32m[0321 09:29:09 @drf_run.py:70][0m Quantizing weight last_linear/W
[32m[0321 09:29:09 @drf_run.py:70][0m Quantizing weight last_linear/b
[32m[0321 09:29:09 @drf_run.py:112][0m Adding activation tensors to summary: [<tf.Tensor 'tower0/last_linear/output:0' shape=(?, 255) dtype=float32>, <tf.Tensor 'tower0/output:0' shape=(?, 255) dtype=float32>]
[32m[0321 09:29:09 @drf_run.py:123][0m Parameter count: {'mults': 2797200, 'weights': 2797710}
[32m[0321 09:29:09 @summary.py:34][0m Maintain moving average summary of 3 tensors.
[32m[0321 09:29:09 @graph.py:90][0m Applying collection UPDATE_OPS of 8 ops.
[32m[0321 09:29:09 @base.py:212][0m Creating the session ...
2018-03-21 09:29:09.732383: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-03-21 09:29:13.618224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:03:00.0
totalMemory: 11.90GiB freeMemory: 11.75GiB
2018-03-21 09:29:13.618283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0, compute capability: 6.1)
[32m[0321 09:29:17 @base.py:220][0m Initializing the session ...
[32m[0321 09:29:17 @base.py:227][0m Graph Finalized.
[32m[0321 09:29:17 @concurrency.py:36][0m Starting EnqueueThread QueueInput/input_queue ...
[32m[0321 09:29:22 @base.py:247][0m Start Epoch 1 ...
  0%|          |0/173481[00:00<?,?it/s] 10%|9         |17134/173481[03:00<27:22,95.19it/s] 10%|#         |18187/173481[03:10<27:11,95.19it/s] 20%|#9        |34217/173481[06:00<24:25,95.05it/s] 20%|##        |35336/173481[06:10<24:13,95.05it/s] 30%|###       |52073/173481[09:00<20:50,97.08it/s] 31%|###       |53159/173481[09:10<20:39,97.08it/s] 40%|####      |70099/173481[12:00<17:28,98.58it/s] 41%|####      |70769/173481[12:10<17:21,98.58it/s] 47%|####7     |82068/173481[15:00<19:11,79.42it/s] 48%|####7     |82770/173481[15:10<19:02,79.42it/s] 54%|#####4    |94129/173481[18:00<18:11,72.67it/s] 55%|#####4    |94867/173481[18:10<18:01,72.67it/s] 61%|######1   |106219/173481[21:00<16:03,69.81it/s] 62%|######1   |106980/173481[21:11<15:52,69.81it/s] 71%|#######1  |123214/173481[24:00<10:26,80.27it/s] 72%|#######1  |124426/173481[24:11<10:11,80.27it/s] 82%|########2 |142732/173481[27:00<05:33,92.24it/s] 83%|########2 |143960/173481[27:11<05:20,92.24it/s] 94%|#########3|162268/173481[30:00<01:52,99.73it/s] 94%|#########4|163526/173481[30:11<01:39,99.73it/s]100%|##########|173481/173481[31:43<00:00,91.14it/s]
[32m[0321 10:01:06 @base.py:257][0m Epoch 1 (global_step 173481) finished, time:1903.38 sec.
[32m[0321 10:01:06 @saver.py:84][0m Model saved to train_log/fcn2_w_8_a_32_quant_ends_True/model-173481.
  0%|          |0/18822[00:00<?,?it/s]100%|##########|18822/18822[02:17<00:00,137.18it/s]
0
[32m[0321 10:03:23 @monitor.py:363][0m QueueInput/queue_size: 49.28
[32m[0321 10:03:23 @monitor.py:363][0m activation-summaries/last_linear/output-rms: 3.8576
[32m[0321 10:03:23 @monitor.py:363][0m activation-summaries/output-rms: 0.028247
[32m[0321 10:03:23 @monitor.py:363][0m cross_entropy_loss: 2.6815
[32m[0321 10:03:23 @monitor.py:363][0m lr: 0.001
[32m[0321 10:03:23 @monitor.py:363][0m param-summary/last_linear/W-rms: 0.10971
[32m[0321 10:03:23 @monitor.py:363][0m param-summary/last_linear/b-rms: 1.0066
[32m[0321 10:03:23 @monitor.py:363][0m param-summary/linear0/W-rms: 0.080036
[32m[0321 10:03:23 @monitor.py:363][0m param-summary/linear0/b-rms: 0.06095
[32m[0321 10:03:23 @monitor.py:363][0m param-summary/linear1/W-rms: 0.085708
[32m[0321 10:03:23 @monitor.py:363][0m param-summary/linear1/b-rms: 0.063621
[32m[0321 10:03:23 @monitor.py:363][0m param-summary/linear2/W-rms: 0.074455
[32m[0321 10:03:23 @monitor.py:363][0m param-summary/linear2/b-rms: 0.066024
[32m[0321 10:03:23 @monitor.py:363][0m param-summary/linear3/W-rms: 0.075004
[32m[0321 10:03:23 @monitor.py:363][0m param-summary/linear3/b-rms: 0.062469
[32m[0321 10:03:23 @monitor.py:363][0m train-error-top1: 0.64848
[32m[0321 10:03:23 @monitor.py:363][0m val-error-top1: 0.74625
[32m[0321 10:03:23 @monitor.py:363][0m val-utt-error: 0.41685
[32m[0321 10:03:23 @monitor.py:363][0m validation_cost: 3.2297
[32m[0321 10:03:23 @monitor.py:363][0m wd_cost: 0.94747
[32m[0321 10:03:23 @group.py:42][0m Callbacks took 137.627 sec in total. InferenceRunner: 137.223sec
[32m[0321 10:03:23 @base.py:247][0m Start Epoch 2 ...
  0%|          |0/173481[00:00<?,?it/s]  8%|8         |14306/173481[03:00<33:22,79.48it/s]  9%|8         |14979/173481[03:10<33:14,79.48it/s] 15%|#5        |26511/173481[06:00<33:28,73.18it/s] 16%|#5        |27243/173481[06:10<33:18,73.18it/s] 22%|##2       |38803/173481[09:00<31:46,70.65it/s] 23%|##2       |39539/173481[09:10<31:35,70.65it/s] 29%|##9       |50932/173481[12:00<29:36,68.97it/s] 30%|##9       |51711/173481[12:10<29:25,68.97it/s] 36%|###6      |63306/173481[15:00<26:40,68.85it/s] 37%|###6      |64012/173481[15:10<26:29,68.85it/s] 43%|####3     |74882/173481[18:00<24:42,66.50it/s] 44%|####3     |75532/173481[18:10<24:32,66.50it/s] 49%|####9     |85591/173481[21:00<23:19,62.80it/s] 50%|####9     |86295/173481[21:11<23:08,62.80it/s] 56%|#####5    |96595/173481[24:00<20:40,61.96it/s] 56%|#####6    |97269/173481[24:11<20:30,61.96it/s] 63%|######2   |108982/173481[27:00<16:29,65.20it/s] 63%|######3   |109774/173481[27:11<16:17,65.20it/s] 70%|#######   |121510/173481[30:00<12:52,67.32it/s] 71%|#######   |122343/173481[30:11<12:39,67.32it/s] 77%|#######7  |134116/173481[33:00<09:33,68.64it/s] 78%|#######7  |134911/173481[33:11<09:21,68.64it/s] 85%|########4 |146711/173481[36:00<06:26,69.30it/s] 85%|########5 |147547/173481[36:11<06:14,69.30it/s] 92%|#########1|159064/173481[39:00<03:29,68.96it/s] 92%|#########2|159860/173481[39:12<03:17,68.96it/s] 99%|#########8|171430/173481[42:00<00:29,68.83it/s] 99%|#########9|172220/173481[42:12<00:18,68.83it/s]100%|##########|173481/173481[42:30<00:00,68.01it/s]
[32m[0321 10:45:54 @base.py:257][0m Epoch 2 (global_step 346962) finished, time:2550.92 sec.
[32m[0321 10:45:55 @saver.py:84][0m Model saved to train_log/fcn2_w_8_a_32_quant_ends_True/model-346962.
[32m[0321 10:45:58 @saver.py:155][0m Model with minimum 'val-error-top1' saved.
  0%|          |0/18822[00:00<?,?it/s]100%|##########|18822/18822[02:36<00:00,120.31it/s]
1
[32m[0321 10:48:35 @monitor.py:363][0m QueueInput/queue_size: 0.63053
[32m[0321 10:48:35 @monitor.py:363][0m activation-summaries/last_linear/output-rms: 3.9217
[32m[0321 10:48:35 @monitor.py:363][0m activation-summaries/output-rms: 0.030187
[32m[0321 10:48:35 @monitor.py:363][0m cross_entropy_loss: 2.5841
[32m[0321 10:48:35 @monitor.py:363][0m lr: 0.001
[32m[0321 10:48:35 @monitor.py:363][0m param-summary/last_linear/W-rms: 0.11089
[32m[0321 10:48:35 @monitor.py:363][0m param-summary/last_linear/b-rms: 1.6559
[32m[0321 10:48:35 @monitor.py:363][0m param-summary/linear0/W-rms: 0.08013
[32m[0321 10:48:35 @monitor.py:363][0m param-summary/linear0/b-rms: 0.06095
[32m[0321 10:48:35 @monitor.py:363][0m param-summary/linear1/W-rms: 0.085866
[32m[0321 10:48:35 @monitor.py:363][0m param-summary/linear1/b-rms: 0.063621
[32m[0321 10:48:35 @monitor.py:363][0m param-summary/linear2/W-rms: 0.073036
[32m[0321 10:48:35 @monitor.py:363][0m param-summary/linear2/b-rms: 0.066024
[32m[0321 10:48:35 @monitor.py:363][0m param-summary/linear3/W-rms: 0.07386
[32m[0321 10:48:35 @monitor.py:363][0m param-summary/linear3/b-rms: 0.062469
[32m[0321 10:48:35 @monitor.py:363][0m train-error-top1: 0.62603
[32m[0321 10:48:35 @monitor.py:363][0m val-error-top1: 0.73899
[32m[0321 10:48:35 @monitor.py:363][0m val-utt-error: 0.41101
[32m[0321 10:48:35 @monitor.py:363][0m validation_cost: 3.1791
[32m[0321 10:48:35 @monitor.py:363][0m wd_cost: 0.94263
[32m[0321 10:48:35 @group.py:42][0m Callbacks took 160.183 sec in total. InferenceRunner: 156.463sec
[32m[0321 10:48:35 @base.py:247][0m Start Epoch 3 ...
  0%|          |0/173481[00:00<?,?it/s]  8%|8         |14560/173481[03:00<32:44,80.89it/s]  9%|8         |15218/173481[03:10<32:36,80.89it/s] 15%|#5        |26384/173481[06:00<33:48,72.50it/s] 16%|#5        |27108/173481[06:10<33:38,72.50it/s] 23%|##2       |39187/173481[09:00<31:10,71.80it/s] 23%|##3       |39921/173481[09:10<31:00,71.80it/s] 30%|##9       |51883/173481[12:00<28:28,71.16it/s] 30%|###       |52636/173481[12:10<28:18,71.16it/s] 37%|###7      |64439/173481[15:00<25:47,70.45it/s] 38%|###7      |65124/173481[15:10<25:38,70.45it/s] 44%|####4     |76597/173481[18:00<23:24,68.96it/s] 45%|####4     |77418/173481[18:10<23:12,68.96it/s] 52%|#####1    |89522/173481[21:00<19:53,70.36it/s] 52%|#####2    |90267/173481[21:11<19:42,70.36it/s] 58%|#####8    |101375/173481[24:00<17:39,68.03it/s] 59%|#####8    |102026/173481[24:11<17:30,68.03it/s] 65%|######4   |112675/173481[27:00<15:31,65.29it/s] 65%|######5   |113436/173481[27:11<15:19,65.29it/s] 72%|#######1  |124681/173481[30:00<12:19,65.97it/s] 72%|#######2  |125443/173481[30:11<12:08,65.97it/s] 79%|#######8  |136315/173481[33:00<09:29,65.29it/s] 79%|#######9  |137079/173481[33:11<09:17,65.29it/s] 85%|########5 |148321/173481[36:00<06:21,65.98it/s] 86%|########5 |149150/173481[36:11<06:08,65.98it/s] 93%|#########2|160894/173481[39:00<03:05,67.86it/s] 93%|#########3|161706/173481[39:12<02:53,67.86it/s]100%|##########|173481/173481[41:55<00:00,68.96it/s]
[32m[0321 11:30:30 @base.py:257][0m Epoch 3 (global_step 520443) finished, time:2515.68 sec.
[32m[0321 11:30:31 @saver.py:84][0m Model saved to train_log/fcn2_w_8_a_32_quant_ends_True/model-520443.
[32m[0321 11:30:32 @saver.py:155][0m Model with minimum 'val-error-top1' saved.
  0%|          |0/18822[00:00<?,?it/s]100%|##########|18822/18822[02:14<00:00,139.53it/s]
2
[32m[0321 11:32:47 @monitor.py:363][0m QueueInput/queue_size: 0.57531
[32m[0321 11:32:47 @monitor.py:363][0m activation-summaries/last_linear/output-rms: 7.8074
[32m[0321 11:32:47 @monitor.py:363][0m activation-summaries/output-rms: 0.03638
[32m[0321 11:32:47 @monitor.py:363][0m cross_entropy_loss: 2.0405
[32m[0321 11:32:47 @monitor.py:363][0m lr: 0.0005
[32m[0321 11:32:47 @monitor.py:363][0m param-summary/last_linear/W-rms: 0.17627
[32m[0321 11:32:47 @monitor.py:363][0m param-summary/last_linear/b-rms: 2.0004
[32m[0321 11:32:47 @monitor.py:363][0m param-summary/linear0/W-rms: 0.11374
[32m[0321 11:32:47 @monitor.py:363][0m param-summary/linear0/b-rms: 0.06095
[32m[0321 11:32:47 @monitor.py:363][0m param-summary/linear1/W-rms: 0.12066
[32m[0321 11:32:47 @monitor.py:363][0m param-summary/linear1/b-rms: 0.063622
[32m[0321 11:32:47 @monitor.py:363][0m param-summary/linear2/W-rms: 0.10737
[32m[0321 11:32:47 @monitor.py:363][0m param-summary/linear2/b-rms: 0.066024
[32m[0321 11:32:47 @monitor.py:363][0m param-summary/linear3/W-rms: 0.11076
[32m[0321 11:32:47 @monitor.py:363][0m param-summary/linear3/b-rms: 0.062469
[32m[0321 11:32:47 @monitor.py:363][0m train-error-top1: 0.5334
[32m[0321 11:32:47 @monitor.py:363][0m val-error-top1: 0.58698
[32m[0321 11:32:47 @monitor.py:363][0m val-utt-error: 0.20758
[32m[0321 11:32:47 @monitor.py:363][0m validation_cost: 2.3354
[32m[0321 11:32:47 @monitor.py:363][0m wd_cost: 0.4051
[32m[0321 11:32:47 @group.py:42][0m Callbacks took 136.616 sec in total. InferenceRunner: 134.917sec
[32m[0321 11:32:47 @base.py:247][0m Start Epoch 4 ...
  0%|          |0/173481[00:00<?,?it/s]  8%|8         |14721/173481[03:00<32:21,81.78it/s]  9%|8         |15451/173481[03:10<32:12,81.78it/s] 16%|#5        |27678/173481[06:00<31:44,76.57it/s] 16%|#6        |28467/173481[06:10<31:33,76.57it/s] 24%|##3       |41033/173481[09:00<29:17,75.36it/s] 24%|##4       |41771/173481[09:10<29:07,75.36it/s] 31%|###1      |54287/173481[12:00<26:40,74.49it/s] 32%|###1      |55047/173481[12:10<26:29,74.49it/s] 39%|###8      |67102/173481[15:00<24:21,72.80it/s] 39%|###9      |67865/173481[15:10<24:10,72.80it/s] 46%|####6     |79825/173481[18:00<21:45,71.73it/s] 46%|####6     |80625/173481[18:10<21:34,71.73it/s] 53%|#####3    |92449/173481[21:00<19:02,70.92it/s] 54%|#####3    |93241/173481[21:11<18:51,70.92it/s] 60%|######    |104796/173481[24:00<16:24,69.74it/s] 61%|######    |105585/173481[24:11<16:13,69.74it/s] 68%|######7   |117132/173481[27:00<13:35,69.13it/s] 68%|######7   |117834/173481[27:11<13:24,69.13it/s] 74%|#######3  |127990/173481[30:00<11:46,64.41it/s] 74%|#######4  |128690/173481[30:11<11:35,64.41it/s] 81%|########  |139987/173481[33:00<08:31,65.51it/s] 81%|########1 |140709/173481[33:11<08:20,65.51it/s] 88%|########8 |152870/173481[36:00<05:01,68.41it/s] 89%|########8 |153690/173481[36:11<04:49,68.41it/s] 96%|#########5|165704/173481[39:00<01:51,69.82it/s] 96%|#########5|166532/173481[39:12<01:39,69.82it/s]100%|##########|173481/173481[40:52<00:00,70.73it/s]
[32m[0321 12:13:40 @base.py:257][0m Epoch 4 (global_step 693924) finished, time:2452.88 sec.
[32m[0321 12:13:40 @saver.py:84][0m Model saved to train_log/fcn2_w_8_a_32_quant_ends_True/model-693924.
[32m[0321 12:13:46 @saver.py:155][0m Model with minimum 'val-error-top1' saved.
  0%|          |0/18822[00:00<?,?it/s]100%|##########|18822/18822[02:22<00:00,132.20it/s]
3
[32m[0321 12:16:09 @monitor.py:363][0m QueueInput/queue_size: 0.5289
[32m[0321 12:16:09 @monitor.py:363][0m activation-summaries/last_linear/output-rms: 9.6912
[32m[0321 12:16:09 @monitor.py:363][0m activation-summaries/output-rms: 0.038892
[32m[0321 12:16:09 @monitor.py:363][0m cross_entropy_loss: 1.8549
[32m[0321 12:16:09 @monitor.py:363][0m lr: 0.0005
[32m[0321 12:16:09 @monitor.py:363][0m param-summary/last_linear/W-rms: 0.19362
[32m[0321 12:16:09 @monitor.py:363][0m param-summary/last_linear/b-rms: 2.0965
[32m[0321 12:16:09 @monitor.py:363][0m param-summary/linear0/W-rms: 0.12017
[32m[0321 12:16:09 @monitor.py:363][0m param-summary/linear0/b-rms: 0.06095
[32m[0321 12:16:09 @monitor.py:363][0m param-summary/linear1/W-rms: 0.13731
[32m[0321 12:16:09 @monitor.py:363][0m param-summary/linear1/b-rms: 0.063622
[32m[0321 12:16:09 @monitor.py:363][0m param-summary/linear2/W-rms: 0.12927
[32m[0321 12:16:09 @monitor.py:363][0m param-summary/linear2/b-rms: 0.066024
[32m[0321 12:16:09 @monitor.py:363][0m param-summary/linear3/W-rms: 0.12891
[32m[0321 12:16:09 @monitor.py:363][0m param-summary/linear3/b-rms: 0.062469
[32m[0321 12:16:09 @monitor.py:363][0m train-error-top1: 0.47641
[32m[0321 12:16:09 @monitor.py:363][0m val-error-top1: 0.54267
[32m[0321 12:16:09 @monitor.py:363][0m val-utt-error: 0.15779
[32m[0321 12:16:09 @monitor.py:363][0m validation_cost: 2.1328
[32m[0321 12:16:09 @monitor.py:363][0m wd_cost: 0.50704
[32m[0321 12:16:09 @group.py:42][0m Callbacks took 149.135 sec in total. InferenceRunner: 142.406sec
[32m[0321 12:16:09 @base.py:247][0m Start Epoch 5 ...
  0%|          |0/173481[00:00<?,?it/s]  8%|7         |13736/173481[03:00<34:53,76.31it/s]  8%|8         |14389/173481[03:10<34:44,76.31it/s] 15%|#4        |25397/173481[06:00<35:13,70.07it/s] 15%|#5        |26076/173481[06:10<35:03,70.07it/s] 22%|##1       |37375/173481[09:00<33:14,68.26it/s] 22%|##1       |38108/173481[09:10<33:03,68.26it/s] 29%|##8       |49555/173481[12:00<30:23,67.95it/s] 29%|##8       |50238/173481[12:10<30:13,67.95it/s] 35%|###5      |61445/173481[15:00<27:52,66.99it/s] 36%|###5      |62163/173481[15:10<27:41,66.99it/s] 42%|####2     |73475/173481[18:00<24:54,66.91it/s] 43%|####2     |74175/173481[18:10<24:44,66.91it/s] 49%|####9     |85412/173481[21:00<22:02,66.61it/s] 50%|####9     |86048/173481[21:11<21:52,66.61it/s] 56%|#####5    |96815/173481[24:00<19:40,64.94it/s] 56%|#####6    |97572/173481[24:11<19:28,64.94it/s] 63%|######2   |108505/173481[27:00<16:40,64.93it/s] 63%|######2   |109242/173481[27:11<16:29,64.93it/s] 69%|######9   |120049/173481[30:00<13:48,64.52it/s] 70%|######9   |120804/173481[30:11<13:36,64.52it/s] 76%|#######5  |131323/173481[33:00<11:03,63.56it/s] 76%|#######6  |132080/173481[33:11<10:51,63.56it/s] 82%|########1 |141853/173481[36:00<08:39,60.92it/s] 82%|########2 |142608/173481[36:11<08:26,60.92it/s] 88%|########7 |152275/173481[39:00<05:57,59.37it/s] 88%|########8 |153012/173481[39:11<05:44,59.37it/s] 94%|#########4|163585/173481[42:00<02:42,61.04it/s] 95%|#########4|164316/173481[42:12<02:30,61.04it/s]100%|##########|173481/173481[44:42<00:00,64.67it/s]
[32m[0321 13:00:52 @base.py:257][0m Epoch 5 (global_step 867405) finished, time:2682.68 sec.
[32m[0321 13:00:53 @saver.py:84][0m Model saved to train_log/fcn2_w_8_a_32_quant_ends_True/model-867405.
[32m[0321 13:00:55 @saver.py:155][0m Model with minimum 'val-error-top1' saved.
  0%|          |0/18822[00:00<?,?it/s]100%|##########|18822/18822[01:54<00:00,164.47it/s]
4
[32m[0321 13:02:50 @monitor.py:363][0m QueueInput/queue_size: 0.5416
[32m[0321 13:02:50 @monitor.py:363][0m activation-summaries/last_linear/output-rms: 9.6695
[32m[0321 13:02:50 @monitor.py:363][0m activation-summaries/output-rms: 0.040403
[32m[0321 13:02:50 @monitor.py:363][0m cross_entropy_loss: 1.7596
[32m[0321 13:02:50 @monitor.py:363][0m lr: 0.0005
[32m[0321 13:02:50 @monitor.py:363][0m param-summary/last_linear/W-rms: 0.19489
[32m[0321 13:02:50 @monitor.py:363][0m param-summary/last_linear/b-rms: 2.1719
[32m[0321 13:02:50 @monitor.py:363][0m param-summary/linear0/W-rms: 0.12128
[32m[0321 13:02:50 @monitor.py:363][0m param-summary/linear0/b-rms: 0.06095
[32m[0321 13:02:50 @monitor.py:363][0m param-summary/linear1/W-rms: 0.14051
[32m[0321 13:02:50 @monitor.py:363][0m param-summary/linear1/b-rms: 0.063622
[32m[0321 13:02:50 @monitor.py:363][0m param-summary/linear2/W-rms: 0.13397
[32m[0321 13:02:50 @monitor.py:363][0m param-summary/linear2/b-rms: 0.066024
[32m[0321 13:02:50 @monitor.py:363][0m param-summary/linear3/W-rms: 0.13219
[32m[0321 13:02:50 @monitor.py:363][0m param-summary/linear3/b-rms: 0.062469
[32m[0321 13:02:50 @monitor.py:363][0m train-error-top1: 0.4571
[32m[0321 13:02:50 @monitor.py:363][0m val-error-top1: 0.52265
[32m[0321 13:02:50 @monitor.py:363][0m val-utt-error: 0.14627
[32m[0321 13:02:50 @monitor.py:363][0m validation_cost: 2.0462
[32m[0321 13:02:50 @monitor.py:363][0m wd_cost: 0.52615
[32m[0321 13:02:50 @group.py:42][0m Callbacks took 118.084 sec in total. InferenceRunner: 114.453sec
[32m[0321 13:02:50 @base.py:247][0m Start Epoch 6 ...
  0%|          |0/173481[00:00<?,?it/s]  7%|6         |11805/173481[03:00<41:05,65.58it/s]  7%|7         |12483/173481[03:10<40:54,65.58it/s] 13%|#3        |23194/173481[06:00<38:53,64.40it/s] 14%|#3        |23895/173481[06:10<38:42,64.40it/s] 20%|#9        |34456/173481[09:00<36:30,63.46it/s] 20%|##        |35067/173481[09:10<36:21,63.46it/s] 26%|##5       |44488/173481[12:00<36:14,59.33it/s] 26%|##5       |45101/173481[12:10<36:03,59.33it/s] 32%|###1      |54954/173481[15:00<33:38,58.73it/s] 32%|###2      |55515/173481[15:10<33:28,58.73it/s] 38%|###8      |65976/173481[18:00<29:53,59.96it/s] 38%|###8      |66651/173481[18:10<29:41,59.96it/s] 44%|####4     |77194/173481[21:00<26:15,61.12it/s] 45%|####4     |77871/173481[21:11<26:04,61.12it/s] 51%|#####     |88264/173481[24:00<23:10,61.30it/s] 51%|#####1    |88974/173481[24:11<22:58,61.30it/s] 57%|#####7    |99410/173481[27:00<20:02,61.61it/s] 58%|#####7    |100101/173481[27:11<19:51,61.61it/s] 64%|######3   |110782/173481[30:00<16:45,62.38it/s] 64%|######4   |111501/173481[30:11<16:33,62.38it/s] 71%|#######   |122385/173481[33:00<13:25,63.40it/s] 71%|#######   |123093/173481[33:11<13:14,63.40it/s] 77%|#######7  |133667/173481[36:00<10:31,63.04it/s] 77%|#######7  |134385/173481[36:11<10:20,63.04it/s] 83%|########3 |144705/173481[39:00<07:42,62.17it/s] 84%|########3 |145431/173481[39:11<07:31,62.17it/s] 90%|########9 |155508/173481[42:00<04:54,61.07it/s] 90%|########9 |156132/173481[42:12<04:44,61.07it/s] 96%|#########5|165952/173481[45:00<02:06,59.50it/s] 96%|#########6|166665/173481[45:12<01:54,59.50it/s]100%|##########|173481/173481[47:11<00:00,61.27it/s]
[32m[0321 13:50:01 @base.py:257][0m Epoch 6 (global_step 1040886) finished, time:2831.62 sec.
[32m[0321 13:50:02 @saver.py:84][0m Model saved to train_log/fcn2_w_8_a_32_quant_ends_True/model-1040886.
[32m[0321 13:50:06 @saver.py:155][0m Model with minimum 'val-error-top1' saved.
  0%|          |0/18822[00:00<?,?it/s]100%|##########|18822/18822[01:40<00:00,187.04it/s]
5
[32m[0321 13:51:46 @monitor.py:363][0m QueueInput/queue_size: 0.7178
[32m[0321 13:51:46 @monitor.py:363][0m activation-summaries/last_linear/output-rms: 17.773
[32m[0321 13:51:46 @monitor.py:363][0m activation-summaries/output-rms: 0.045966
[32m[0321 13:51:46 @monitor.py:363][0m cross_entropy_loss: 1.3495
[32m[0321 13:51:46 @monitor.py:363][0m lr: 0.00025
[32m[0321 13:51:46 @monitor.py:363][0m param-summary/last_linear/W-rms: 0.2561
[32m[0321 13:51:46 @monitor.py:363][0m param-summary/last_linear/b-rms: 2.2121
[32m[0321 13:51:46 @monitor.py:363][0m param-summary/linear0/W-rms: 0.15639
[32m[0321 13:51:46 @monitor.py:363][0m param-summary/linear0/b-rms: 0.06095
[32m[0321 13:51:46 @monitor.py:363][0m param-summary/linear1/W-rms: 0.17119
[32m[0321 13:51:46 @monitor.py:363][0m param-summary/linear1/b-rms: 0.063622
[32m[0321 13:51:46 @monitor.py:363][0m param-summary/linear2/W-rms: 0.16851
[32m[0321 13:51:46 @monitor.py:363][0m param-summary/linear2/b-rms: 0.066024
[32m[0321 13:51:46 @monitor.py:363][0m param-summary/linear3/W-rms: 0.16615
[32m[0321 13:51:46 @monitor.py:363][0m param-summary/linear3/b-rms: 0.062469
[32m[0321 13:51:46 @monitor.py:363][0m train-error-top1: 0.36274
[32m[0321 13:51:46 @monitor.py:363][0m val-error-top1: 0.41777
[32m[0321 13:51:46 @monitor.py:363][0m val-utt-error: 0.079056
[32m[0321 13:51:46 @monitor.py:363][0m validation_cost: 1.568
[32m[0321 13:51:46 @monitor.py:363][0m wd_cost: 0.16969
[32m[0321 13:51:46 @group.py:42][0m Callbacks took 105.051 sec in total. InferenceRunner: 100.650sec
[32m[0321 13:51:46 @base.py:247][0m Start Epoch 7 ...
  0%|          |0/173481[00:00<?,?it/s]  8%|7         |13609/173481[03:00<35:16,75.53it/s]  9%|8         |15066/173481[03:20<34:57,75.53it/s] 16%|#5        |27026/173481[06:00<32:31,75.03it/s] 16%|#6        |27811/173481[06:10<32:21,75.03it/s] 23%|##3       |40574/173481[09:00<29:28,75.15it/s] 24%|##3       |41388/173481[09:10<29:17,75.15it/s] 31%|###1      |54198/173481[12:00<26:21,75.42it/s] 32%|###1      |54978/173481[12:10<26:11,75.42it/s] 39%|###8      |67489/173481[15:00<23:40,74.62it/s] 39%|###9      |68304/173481[15:10<23:29,74.62it/s] 47%|####6     |81118/173481[18:00<20:28,75.16it/s] 47%|####7     |81864/173481[18:10<20:18,75.16it/s] 54%|#####4    |93799/173481[21:00<18:15,72.73it/s] 54%|#####4    |94530/173481[21:11<18:05,72.73it/s] 62%|######1   |106761/173481[24:00<15:21,72.37it/s] 62%|######1   |107544/173481[24:11<15:11,72.37it/s] 69%|######8   |119701/173481[27:00<12:25,72.12it/s] 69%|######9   |120483/173481[27:11<12:14,72.12it/s] 76%|#######6  |132187/173481[30:00<09:43,70.72it/s] 77%|#######6  |133014/173481[30:11<09:32,70.72it/s] 82%|########2 |143095/173481[33:03<07:49,64.67it/s] 83%|########2 |143442/173481[33:21<07:44,64.67it/s] 89%|########8 |154219/173481[36:03<05:04,63.20it/s] 90%|########9 |155592/173481[36:21<04:43,63.20it/s] 96%|#########6|166585/173481[39:03<01:44,65.84it/s] 97%|#########6|167724/173481[39:22<01:27,65.84it/s]100%|##########|173481/173481[40:49<00:00,70.82it/s]
[32m[0321 14:32:36 @base.py:257][0m Epoch 7 (global_step 1214367) finished, time:2449.59 sec.
[32m[0321 14:32:37 @saver.py:84][0m Model saved to train_log/fcn2_w_8_a_32_quant_ends_True/model-1214367.
[32m[0321 14:32:38 @saver.py:155][0m Model with minimum 'val-error-top1' saved.
  0%|          |0/18822[00:00<?,?it/s]100%|##########|18822/18822[01:48<00:00,173.11it/s]
6
[32m[0321 14:34:27 @monitor.py:363][0m QueueInput/queue_size: 0.55057
[32m[0321 14:34:27 @monitor.py:363][0m activation-summaries/last_linear/output-rms: 25.814
[32m[0321 14:34:27 @monitor.py:363][0m activation-summaries/output-rms: 0.045893
[32m[0321 14:34:27 @monitor.py:363][0m cross_entropy_loss: 1.3404
[32m[0321 14:34:27 @monitor.py:363][0m lr: 0.00025
[32m[0321 14:34:27 @monitor.py:363][0m param-summary/last_linear/W-rms: 0.31709
[32m[0321 14:34:27 @monitor.py:363][0m param-summary/last_linear/b-rms: 2.2281
[32m[0321 14:34:27 @monitor.py:363][0m param-summary/linear0/W-rms: 0.17214
[32m[0321 14:34:27 @monitor.py:363][0m param-summary/linear0/b-rms: 0.06095
[32m[0321 14:34:27 @monitor.py:363][0m param-summary/linear1/W-rms: 0.19918
[32m[0321 14:34:27 @monitor.py:363][0m param-summary/linear1/b-rms: 0.063622
[32m[0321 14:34:27 @monitor.py:363][0m param-summary/linear2/W-rms: 0.19878
[32m[0321 14:34:27 @monitor.py:363][0m param-summary/linear2/b-rms: 0.066024
[32m[0321 14:34:27 @monitor.py:363][0m param-summary/linear3/W-rms: 0.19536
[32m[0321 14:34:27 @monitor.py:363][0m param-summary/linear3/b-rms: 0.062469
[32m[0321 14:34:27 @monitor.py:363][0m train-error-top1: 0.35968
[32m[0321 14:34:27 @monitor.py:363][0m val-error-top1: 0.3943
[32m[0321 14:34:27 @monitor.py:363][0m val-utt-error: 0.065933
[32m[0321 14:34:27 @monitor.py:363][0m validation_cost: 1.4711
[32m[0321 14:34:27 @monitor.py:363][0m wd_cost: 0.23066
[32m[0321 14:34:27 @group.py:42][0m Callbacks took 110.753 sec in total. InferenceRunner: 108.743sec
[32m[0321 14:34:27 @base.py:247][0m Start Epoch 8 ...
  0%|          |0/173481[00:00<?,?it/s]  7%|7         |12673/173481[03:00<38:04,70.41it/s]  8%|7         |13335/173481[03:10<37:54,70.41it/s] 14%|#4        |24658/173481[06:00<36:14,68.43it/s] 15%|#4        |25353/173481[06:10<36:04,68.43it/s] 22%|##1       |37547/173481[09:00<32:22,69.98it/s] 22%|##2       |38600/173481[09:10<32:07,69.98it/s] 33%|###2      |56812/173481[12:00<22:58,84.63it/s] 33%|###3      |57947/173481[12:10<22:45,84.63it/s] 44%|####3     |76222/173481[15:00<17:05,94.83it/s] 45%|####4     |77363/173481[15:10<16:53,94.83it/s] 55%|#####5    |95572/173481[18:00<12:53,100.77it/s] 56%|#####5    |96741/173481[18:10<12:41,100.77it/s] 66%|######6   |115126/173481[21:00<09:18,104.55it/s] 67%|######7   |116322/173481[21:11<09:06,104.55it/s] 76%|#######6  |132490/173481[24:00<06:48,100.34it/s] 77%|#######6  |133239/173481[24:11<06:41,100.34it/s] 83%|########3 |144544/173481[27:00<06:00,80.32it/s]  84%|########3 |145294/173481[27:11<05:50,80.32it/s] 90%|######### |156683/173481[30:00<03:49,73.32it/s] 91%|######### |157475/173481[30:11<03:38,73.32it/s] 97%|#########7|168887/173481[33:00<01:05,70.45it/s] 98%|#########7|169650/173481[33:11<00:54,70.45it/s]100%|##########|173481/173481[34:09<00:00,84.66it/s]
[32m[0321 15:08:36 @base.py:257][0m Epoch 8 (global_step 1387848) finished, time:2049.15 sec.
[32m[0321 15:08:36 @saver.py:84][0m Model saved to train_log/fcn2_w_8_a_32_quant_ends_True/model-1387848.
[32m[0321 15:08:38 @saver.py:155][0m Model with minimum 'val-error-top1' saved.
  0%|          |0/18822[00:00<?,?it/s]100%|##########|18822/18822[01:46<00:00,176.88it/s]
7
[32m[0321 15:10:25 @monitor.py:363][0m QueueInput/queue_size: 0.6973
[32m[0321 15:10:25 @monitor.py:363][0m activation-summaries/last_linear/output-rms: 31.077
[32m[0321 15:10:25 @monitor.py:363][0m activation-summaries/output-rms: 0.046689
[32m[0321 15:10:25 @monitor.py:363][0m cross_entropy_loss: 1.2053
[32m[0321 15:10:25 @monitor.py:363][0m lr: 0.00025
[32m[0321 15:10:25 @monitor.py:363][0m param-summary/last_linear/W-rms: 0.34602
[32m[0321 15:10:25 @monitor.py:363][0m param-summary/last_linear/b-rms: 2.2445
[32m[0321 15:10:25 @monitor.py:363][0m param-summary/linear0/W-rms: 0.17519
[32m[0321 15:10:25 @monitor.py:363][0m param-summary/linear0/b-rms: 0.06095
[32m[0321 15:10:25 @monitor.py:363][0m param-summary/linear1/W-rms: 0.20761
[32m[0321 15:10:25 @monitor.py:363][0m param-summary/linear1/b-rms: 0.063622
[32m[0321 15:10:25 @monitor.py:363][0m param-summary/linear2/W-rms: 0.208
[32m[0321 15:10:25 @monitor.py:363][0m param-summary/linear2/b-rms: 0.066024
[32m[0321 15:10:25 @monitor.py:363][0m param-summary/linear3/W-rms: 0.2036
[32m[0321 15:10:25 @monitor.py:363][0m param-summary/linear3/b-rms: 0.062469
[32m[0321 15:10:25 @monitor.py:363][0m train-error-top1: 0.32332
[32m[0321 15:10:25 @monitor.py:363][0m val-error-top1: 0.38595
[32m[0321 15:10:25 @monitor.py:363][0m val-utt-error: 0.063171
[32m[0321 15:10:25 @monitor.py:363][0m validation_cost: 1.4365
[32m[0321 15:10:25 @monitor.py:363][0m wd_cost: 0.2533
[32m[0321 15:10:25 @group.py:42][0m Callbacks took 108.832 sec in total. InferenceRunner: 106.424sec
[32m[0321 15:10:25 @base.py:247][0m Start Epoch 9 ...
  0%|          |0/173481[00:00<?,?it/s] 11%|#         |18917/173481[03:00<24:30,105.09it/s] 11%|#1        |19614/173481[03:10<24:24,105.09it/s] 18%|#8        |31293/173481[06:00<28:30,83.13it/s]  18%|#8        |32013/173481[06:10<28:21,83.13it/s] 25%|##5       |43765/173481[09:00<28:36,75.56it/s] 26%|##5       |44478/173481[09:10<28:27,75.56it/s] 32%|###2      |56047/173481[12:00<27:17,71.71it/s] 33%|###2      |56764/173481[12:10<27:07,71.71it/s] 40%|###9      |68791/173481[15:00<24:29,71.25it/s] 40%|####      |69522/173481[15:10<24:19,71.25it/s] 47%|####6     |80949/173481[18:00<22:14,69.35it/s] 47%|####7     |81750/173481[18:10<22:02,69.35it/s] 54%|#####4    |93985/173481[21:00<18:42,70.85it/s] 55%|#####4    |94734/173481[21:11<18:31,70.85it/s] 61%|######    |105796/173481[24:00<16:33,68.13it/s] 61%|######1   |106498/173481[24:11<16:23,68.13it/s] 68%|######7   |117439/173481[27:00<14:04,66.36it/s] 68%|######8   |118134/173481[27:11<13:54,66.36it/s] 74%|#######4  |128965/173481[30:00<11:23,65.17it/s] 75%|#######4  |129696/173481[30:11<11:11,65.17it/s] 80%|########  |139407/173481[33:00<09:15,61.38it/s] 81%|########1 |140648/173481[33:11<08:54,61.38it/s] 88%|########8 |153319/173481[36:00<04:54,68.41it/s] 89%|########8 |154112/173481[36:11<04:43,68.41it/s] 96%|#########5|166363/173481[39:00<01:41,70.38it/s] 96%|#########6|167232/173481[39:11<01:28,70.38it/s]100%|##########|173481/173481[40:38<00:00,71.13it/s]
[32m[0321 15:51:04 @base.py:257][0m Epoch 9 (global_step 1561329) finished, time:2438.94 sec.
[32m[0321 15:51:04 @saver.py:84][0m Model saved to train_log/fcn2_w_8_a_32_quant_ends_True/model-1561329.
[32m[0321 15:51:05 @saver.py:155][0m Model with minimum 'val-error-top1' saved.
  0%|          |0/18822[00:00<?,?it/s]100%|##########|18822/18822[01:49<00:00,172.51it/s]
8
[32m[0321 15:52:54 @monitor.py:363][0m QueueInput/queue_size: 0.69706
[32m[0321 15:52:54 @monitor.py:363][0m activation-summaries/last_linear/output-rms: 40.49
[32m[0321 15:52:54 @monitor.py:363][0m activation-summaries/output-rms: 0.048666
[32m[0321 15:52:54 @monitor.py:363][0m cross_entropy_loss: 1.115
[32m[0321 15:52:54 @monitor.py:363][0m lr: 0.000125
[32m[0321 15:52:54 @monitor.py:363][0m param-summary/last_linear/W-rms: 0.397
[32m[0321 15:52:54 @monitor.py:363][0m param-summary/last_linear/b-rms: 2.2496
[32m[0321 15:52:54 @monitor.py:363][0m param-summary/linear0/W-rms: 0.19605
[32m[0321 15:52:54 @monitor.py:363][0m param-summary/linear0/b-rms: 0.06095
[32m[0321 15:52:54 @monitor.py:363][0m param-summary/linear1/W-rms: 0.21878
[32m[0321 15:52:54 @monitor.py:363][0m param-summary/linear1/b-rms: 0.063622
[32m[0321 15:52:54 @monitor.py:363][0m param-summary/linear2/W-rms: 0.21899
[32m[0321 15:52:54 @monitor.py:363][0m param-summary/linear2/b-rms: 0.066024
[32m[0321 15:52:54 @monitor.py:363][0m param-summary/linear3/W-rms: 0.21454
[32m[0321 15:52:54 @monitor.py:363][0m param-summary/linear3/b-rms: 0.062469
[32m[0321 15:52:54 @monitor.py:363][0m train-error-top1: 0.31261
[32m[0321 15:52:54 @monitor.py:363][0m val-error-top1: 0.33535
[32m[0321 15:52:54 @monitor.py:363][0m val-utt-error: 0.042291
[32m[0321 15:52:54 @monitor.py:363][0m validation_cost: 1.2284
[32m[0321 15:52:54 @monitor.py:363][0m wd_cost: 0.060526
[32m[0321 15:52:54 @group.py:42][0m Callbacks took 110.868 sec in total. InferenceRunner: 109.126sec
[32m[0321 15:52:54 @base.py:247][0m Start Epoch 10 ...
  0%|          |0/173481[00:00<?,?it/s] 12%|#1        |19999/173481[03:00<23:01,111.10it/s] 12%|#1        |20721/173481[03:10<22:54,111.10it/s] 19%|#8        |32891/173481[06:00<26:54,87.10it/s]  19%|#9        |33659/173481[06:10<26:45,87.10it/s] 26%|##6       |45782/173481[09:00<27:04,78.60it/s] 27%|##6       |46535/173481[09:10<26:55,78.60it/s] 34%|###3      |58450/173481[12:00<25:49,74.25it/s] 34%|###4      |59155/173481[12:10<25:39,74.25it/s] 41%|####1     |71278/173481[15:00<23:25,72.72it/s] 42%|####1     |72046/173481[15:10<23:14,72.72it/s] 49%|####8     |84442/173481[18:00<20:21,72.92it/s] 49%|####9     |85209/173481[18:10<20:10,72.92it/s] 56%|#####6    |97606/173481[21:00<17:19,73.02it/s] 57%|#####6    |98487/173481[21:11<17:07,73.02it/s] 66%|######5   |113724/173481[24:00<12:22,80.44it/s] 66%|######6   |114878/173481[24:11<12:08,80.44it/s] 77%|#######6  |133303/173481[27:00<07:14,92.49it/s] 78%|#######7  |134556/173481[27:11<07:00,92.49it/s] 86%|########5 |148798/173481[30:00<04:36,89.16it/s] 86%|########6 |149634/173481[30:11<04:27,89.16it/s] 93%|#########2|161164/173481[33:00<02:38,77.60it/s] 93%|#########3|161975/173481[33:11<02:28,77.60it/s]100%|#########9|173122/173481[36:00<00:05,71.57it/s]100%|##########|173481/173481[36:05<00:00,80.11it/s]
[32m[0321 16:29:00 @base.py:257][0m Epoch 10 (global_step 1734810) finished, time:2165.63 sec.
[32m[0321 16:29:01 @saver.py:84][0m Model saved to train_log/fcn2_w_8_a_32_quant_ends_True/model-1734810.
[32m[0321 16:29:02 @saver.py:155][0m Model with minimum 'val-error-top1' saved.
  0%|          |0/18822[00:00<?,?it/s]100%|##########|18822/18822[02:10<00:00,144.45it/s]
9
[32m[0321 16:31:12 @monitor.py:363][0m QueueInput/queue_size: 0.75117
[32m[0321 16:31:12 @monitor.py:363][0m activation-summaries/last_linear/output-rms: 47.774
[32m[0321 16:31:12 @monitor.py:363][0m activation-summaries/output-rms: 0.048277
[32m[0321 16:31:12 @monitor.py:363][0m cross_entropy_loss: 1.1104
[32m[0321 16:31:12 @monitor.py:363][0m lr: 0.000125
[32m[0321 16:31:12 @monitor.py:363][0m param-summary/last_linear/W-rms: 0.45158
[32m[0321 16:31:12 @monitor.py:363][0m param-summary/last_linear/b-rms: 2.2525
[32m[0321 16:31:12 @monitor.py:363][0m param-summary/linear0/W-rms: 0.21334
[32m[0321 16:31:12 @monitor.py:363][0m param-summary/linear0/b-rms: 0.06095
[32m[0321 16:31:12 @monitor.py:363][0m param-summary/linear1/W-rms: 0.23423
[32m[0321 16:31:12 @monitor.py:363][0m param-summary/linear1/b-rms: 0.063622
[32m[0321 16:31:12 @monitor.py:363][0m param-summary/linear2/W-rms: 0.23502
[32m[0321 16:31:12 @monitor.py:363][0m param-summary/linear2/b-rms: 0.066024
[32m[0321 16:31:12 @monitor.py:363][0m param-summary/linear3/W-rms: 0.23037
[32m[0321 16:31:12 @monitor.py:363][0m param-summary/linear3/b-rms: 0.062469
[32m[0321 16:31:12 @monitor.py:363][0m train-error-top1: 0.3011
[32m[0321 16:31:12 @monitor.py:363][0m val-error-top1: 0.32992
[32m[0321 16:31:12 @monitor.py:363][0m val-utt-error: 0.039528
[32m[0321 16:31:12 @monitor.py:363][0m validation_cost: 1.2069
[32m[0321 16:31:12 @monitor.py:363][0m wd_cost: 0.072475
[32m[0321 16:31:12 @group.py:42][0m Callbacks took 132.102 sec in total. InferenceRunner: 130.312sec
[32m[0321 16:31:12 @base.py:247][0m Start Epoch 11 ...
  0%|          |0/173481[00:00<?,?it/s] 12%|#1        |19995/173481[03:00<23:01,111.08it/s] 12%|#2        |21110/173481[03:10<22:51,111.08it/s] 20%|#9        |34253/173481[06:00<25:05,92.48it/s]  20%|##        |34870/173481[06:10<24:58,92.48it/s] 26%|##6       |45871/173481[09:00<27:58,76.02it/s] 27%|##6       |46660/173481[09:10<27:48,76.02it/s] 34%|###3      |58745/173481[12:00<25:56,73.70it/s] 34%|###4      |59456/173481[12:10<25:47,73.70it/s] 41%|####1     |71367/173481[15:00<23:40,71.87it/s] 42%|####1     |72138/173481[15:10<23:30,71.87it/s] 48%|####8     |83953/173481[18:00<21:03,70.87it/s] 49%|####8     |84732/173481[18:10<20:52,70.87it/s] 56%|#####5    |96679/173481[21:00<18:05,70.78it/s] 56%|#####6    |97480/173481[21:11<17:53,70.78it/s] 63%|######3   |109657/173481[24:00<14:53,71.43it/s] 64%|######3   |110424/173481[24:11<14:42,71.43it/s] 70%|#######   |121585/173481[27:00<12:34,68.74it/s] 70%|#######   |121860/173481[27:11<12:30,68.74it/s] 78%|#######7  |134647/173481[30:00<09:10,70.58it/s] 78%|#######8  |135373/173481[30:11<08:59,70.58it/s] 85%|########4 |146995/173481[33:00<06:20,69.57it/s] 85%|########5 |147723/173481[33:11<06:10,69.57it/s] 92%|#########1|159204/173481[36:00<03:27,68.69it/s] 92%|#########2|159957/173481[36:11<03:16,68.69it/s] 99%|#########8|171133/173481[39:00<00:34,67.45it/s] 99%|#########9|171948/173481[39:12<00:22,67.45it/s]100%|##########|173481/173481[39:33<00:00,73.08it/s]
[32m[0321 17:10:46 @base.py:257][0m Epoch 11 (global_step 1908291) finished, time:2373.76 sec.
[32m[0321 17:10:47 @saver.py:84][0m Model saved to train_log/fcn2_w_8_a_32_quant_ends_True/model-1908291.
[32m[0321 17:10:48 @saver.py:155][0m Model with minimum 'val-error-top1' saved.
  0%|          |0/18822[00:00<?,?it/s]100%|##########|18822/18822[01:52<00:00,167.95it/s]
10
[32m[0321 17:12:40 @monitor.py:363][0m QueueInput/queue_size: 0.4779
[32m[0321 17:12:40 @monitor.py:363][0m activation-summaries/last_linear/output-rms: 54.912
[32m[0321 17:12:40 @monitor.py:363][0m activation-summaries/output-rms: 0.049512
[32m[0321 17:12:40 @monitor.py:363][0m cross_entropy_loss: 1.0449
[32m[0321 17:12:40 @monitor.py:363][0m lr: 6.25e-05
[32m[0321 17:12:40 @monitor.py:363][0m param-summary/last_linear/W-rms: 0.49364
[32m[0321 17:12:40 @monitor.py:363][0m param-summary/last_linear/b-rms: 2.2559
[32m[0321 17:12:40 @monitor.py:363][0m param-summary/linear0/W-rms: 0.22054
[32m[0321 17:12:40 @monitor.py:363][0m param-summary/linear0/b-rms: 0.06095
[32m[0321 17:12:40 @monitor.py:363][0m param-summary/linear1/W-rms: 0.24421
[32m[0321 17:12:40 @monitor.py:363][0m param-summary/linear1/b-rms: 0.063622
[32m[0321 17:12:40 @monitor.py:363][0m param-summary/linear2/W-rms: 0.24588
[32m[0321 17:12:40 @monitor.py:363][0m param-summary/linear2/b-rms: 0.066024
[32m[0321 17:12:40 @monitor.py:363][0m param-summary/linear3/W-rms: 0.24112
[32m[0321 17:12:40 @monitor.py:363][0m param-summary/linear3/b-rms: 0.062469
[32m[0321 17:12:40 @monitor.py:363][0m train-error-top1: 0.28946
[32m[0321 17:12:40 @monitor.py:363][0m val-error-top1: 0.30948
[32m[0321 17:12:40 @monitor.py:363][0m val-utt-error: 0.034693
[32m[0321 17:12:40 @monitor.py:363][0m validation_cost: 1.1199
[32m[0321 17:12:40 @monitor.py:363][0m wd_cost: 0.080883
[32m[0321 17:12:40 @group.py:42][0m Callbacks took 114.011 sec in total. InferenceRunner: 112.092sec
[32m[0321 17:12:40 @base.py:247][0m Start Epoch 12 ...
  0%|          |0/173481[00:00<?,?it/s] 12%|#1        |20008/173481[03:00<23:00,111.14it/s] 12%|#1        |20733/173481[03:10<22:54,111.14it/s] 19%|#8        |32746/173481[06:00<27:07,86.46it/s]  19%|#9        |33469/173481[06:10<26:59,86.46it/s] 26%|##6       |45435/173481[09:00<27:28,77.66it/s] 27%|##6       |46179/173481[09:10<27:19,77.66it/s] 33%|###3      |57869/173481[12:00<26:21,73.12it/s] 34%|###3      |58620/173481[12:10<26:10,73.12it/s] 41%|####      |70606/173481[15:00<23:50,71.92it/s] 41%|####1     |71393/173481[15:10<23:39,71.92it/s] 48%|####8     |83284/173481[18:00<21:07,71.16it/s] 48%|####8     |84015/173481[18:10<20:57,71.16it/s] 55%|#####5    |95658/173481[21:00<18:32,69.93it/s] 56%|#####5    |96417/173481[21:11<18:22,69.93it/s] 62%|######1   |107304/173481[24:00<16:24,67.21it/s] 62%|######2   |108069/173481[24:11<16:13,67.21it/s] 69%|######8   |119171/173481[27:00<13:35,66.56it/s] 69%|######9   |119920/173481[27:11<13:24,66.56it/s] 75%|#######4  |129651/173481[30:00<11:45,62.11it/s] 75%|#######5  |130412/173481[30:11<11:33,62.11it/s] 82%|########1 |141928/173481[33:00<08:05,65.01it/s] 82%|########2 |142659/173481[33:11<07:54,65.01it/s] 89%|########8 |154229/173481[36:00<04:48,66.63it/s] 89%|########9 |155049/173481[36:11<04:36,66.63it/s] 96%|#########6|166684/173481[39:00<01:40,67.89it/s] 97%|#########6|167523/173481[39:12<01:27,67.89it/s]100%|##########|173481/173481[40:38<00:00,71.15it/s]
[32m[0321 17:53:18 @base.py:257][0m Epoch 12 (global_step 2081772) finished, time:2438.09 sec.
[32m[0321 17:53:19 @saver.py:84][0m Model saved to train_log/fcn2_w_8_a_32_quant_ends_True/model-2081772.
[32m[0321 17:53:20 @saver.py:155][0m Model with minimum 'val-error-top1' saved.
  0%|          |0/18822[00:00<?,?it/s]100%|##########|18822/18822[02:09<00:00,145.44it/s]
11
[32m[0321 17:55:30 @monitor.py:363][0m QueueInput/queue_size: 0.93072
[32m[0321 17:55:30 @monitor.py:363][0m activation-summaries/last_linear/output-rms: 61.868
[32m[0321 17:55:30 @monitor.py:363][0m activation-summaries/output-rms: 0.05038
[32m[0321 17:55:30 @monitor.py:363][0m cross_entropy_loss: 0.96348
[32m[0321 17:55:30 @monitor.py:363][0m lr: 6.25e-05
[32m[0321 17:55:30 @monitor.py:363][0m param-summary/last_linear/W-rms: 0.52885
[32m[0321 17:55:30 @monitor.py:363][0m param-summary/last_linear/b-rms: 2.2567
[32m[0321 17:55:30 @monitor.py:363][0m param-summary/linear0/W-rms: 0.23318
[32m[0321 17:55:30 @monitor.py:363][0m param-summary/linear0/b-rms: 0.06095
[32m[0321 17:55:30 @monitor.py:363][0m param-summary/linear1/W-rms: 0.2493
[32m[0321 17:55:30 @monitor.py:363][0m param-summary/linear1/b-rms: 0.063622
[32m[0321 17:55:30 @monitor.py:363][0m param-summary/linear2/W-rms: 0.25073
[32m[0321 17:55:30 @monitor.py:363][0m param-summary/linear2/b-rms: 0.066024
[32m[0321 17:55:30 @monitor.py:363][0m param-summary/linear3/W-rms: 0.24594
[32m[0321 17:55:30 @monitor.py:363][0m param-summary/linear3/b-rms: 0.062469
[32m[0321 17:55:30 @monitor.py:363][0m train-error-top1: 0.28125
[32m[0321 17:55:30 @monitor.py:363][0m val-error-top1: 0.30197
[32m[0321 17:55:30 @monitor.py:363][0m val-utt-error: 0.032196
[32m[0321 17:55:30 @monitor.py:363][0m validation_cost: 1.0917
[32m[0321 17:55:30 @monitor.py:363][0m wd_cost: 0.017675
[32m[0321 17:55:30 @group.py:42][0m Callbacks took 131.574 sec in total. InferenceRunner: 129.434sec
[32m[0321 17:55:30 @base.py:247][0m Start Epoch 13 ...
  0%|          |0/173481[00:00<?,?it/s]  7%|6         |11875/173481[03:00<40:50,65.96it/s]  7%|7         |12516/173481[03:10<40:40,65.96it/s] 13%|#2        |22477/173481[06:00<40:26,62.23it/s] 13%|#3        |23064/173481[06:10<40:17,62.23it/s] 19%|#9        |33685/173481[09:00<37:26,62.24it/s] 20%|#9        |34358/173481[09:10<37:15,62.24it/s] 26%|##5       |44911/173481[12:00<34:23,62.30it/s] 26%|##6       |45582/173481[12:10<34:13,62.30it/s] 32%|###2      |55915/173481[15:00<31:45,61.70it/s] 33%|###2      |56586/173481[15:10<31:34,61.70it/s] 39%|###8      |66870/173481[18:00<28:59,61.28it/s] 39%|###8      |67608/173481[18:10<28:47,61.28it/s] 45%|####5     |78841/173481[21:00<24:43,63.78it/s] 46%|####5     |79584/173481[21:11<24:32,63.78it/s] 52%|#####1    |90044/173481[24:00<22:04,63.00it/s] 52%|#####2    |90727/173481[24:11<21:53,63.00it/s] 58%|#####8    |101329/173481[27:00<19:08,62.84it/s] 59%|#####8    |101988/173481[27:11<18:57,62.84it/s] 65%|######4   |112423/173481[30:00<16:21,62.22it/s] 65%|######5   |113100/173481[30:11<16:10,62.22it/s] 71%|#######1  |123193/173481[33:00<13:44,61.00it/s] 71%|#######1  |123879/173481[33:11<13:33,61.00it/s] 77%|#######7  |134155/173481[36:00<10:45,60.93it/s] 78%|#######7  |134820/173481[36:11<10:34,60.93it/s] 84%|########3 |145063/173481[39:00<07:47,60.76it/s] 84%|########4 |145808/173481[39:11<07:35,60.76it/s] 90%|######### |156475/173481[42:00<04:34,62.04it/s] 91%|######### |157191/173481[42:12<04:22,62.04it/s] 97%|#########6|167581/173481[45:00<01:35,61.87it/s] 97%|#########7|168305/173481[45:12<01:23,61.87it/s]100%|##########|173481/173481[46:30<00:00,62.17it/s]
[32m[0321 18:42:00 @base.py:257][0m Epoch 13 (global_step 2255253) finished, time:2790.27 sec.
[32m[0321 18:42:01 @saver.py:84][0m Model saved to train_log/fcn2_w_8_a_32_quant_ends_True/model-2255253.
[32m[0321 18:42:02 @saver.py:155][0m Model with minimum 'val-error-top1' saved.
  0%|          |0/18822[00:00<?,?it/s]100%|##########|18822/18822[02:30<00:00,125.48it/s]
12
[32m[0321 18:44:32 @monitor.py:363][0m QueueInput/queue_size: 0.46529
[32m[0321 18:44:32 @monitor.py:363][0m activation-summaries/last_linear/output-rms: 67.021
[32m[0321 18:44:32 @monitor.py:363][0m activation-summaries/output-rms: 0.049565
[32m[0321 18:44:32 @monitor.py:363][0m cross_entropy_loss: 1.0267
[32m[0321 18:44:32 @monitor.py:363][0m lr: 6.25e-05
[32m[0321 18:44:32 @monitor.py:363][0m param-summary/last_linear/W-rms: 0.56319
[32m[0321 18:44:32 @monitor.py:363][0m param-summary/last_linear/b-rms: 2.2577
[32m[0321 18:44:32 @monitor.py:363][0m param-summary/linear0/W-rms: 0.24356
[32m[0321 18:44:32 @monitor.py:363][0m param-summary/linear0/b-rms: 0.06095
[32m[0321 18:44:32 @monitor.py:363][0m param-summary/linear1/W-rms: 0.25471
[32m[0321 18:44:32 @monitor.py:363][0m param-summary/linear1/b-rms: 0.063622
[32m[0321 18:44:32 @monitor.py:363][0m param-summary/linear2/W-rms: 0.25594
[32m[0321 18:44:32 @monitor.py:363][0m param-summary/linear2/b-rms: 0.066024
[32m[0321 18:44:32 @monitor.py:363][0m param-summary/linear3/W-rms: 0.25111
[32m[0321 18:44:32 @monitor.py:363][0m param-summary/linear3/b-rms: 0.062469
[32m[0321 18:44:32 @monitor.py:363][0m train-error-top1: 0.29125
[32m[0321 18:44:32 @monitor.py:363][0m val-error-top1: 0.29871
[32m[0321 18:44:32 @monitor.py:363][0m val-utt-error: 0.03209
[32m[0321 18:44:32 @monitor.py:363][0m validation_cost: 1.0814
[32m[0321 18:44:32 @monitor.py:363][0m wd_cost: 0.019168
[32m[0321 18:44:32 @group.py:42][0m Callbacks took 152.110 sec in total. InferenceRunner: 150.012sec
[32m[0321 18:44:32 @base.py:247][0m Start Epoch 14 ...
  0%|          |0/173481[00:00<?,?it/s] 10%|#         |18208/173481[03:00<25:35,101.14it/s] 11%|#         |18778/173481[03:10<25:29,101.14it/s] 16%|#6        |28381/173481[06:00<33:21,72.51it/s]  17%|#6        |28959/173481[06:10<33:13,72.51it/s] 22%|##2       |38914/173481[09:00<34:37,64.76it/s] 23%|##2       |39555/173481[09:10<34:27,64.76it/s] 29%|##8       |50044/173481[12:00<32:31,63.26it/s] 29%|##9       |50655/173481[12:10<32:21,63.26it/s] 35%|###5      |61168/173481[15:00<29:56,62.52it/s] 36%|###5      |61838/173481[15:10<29:45,62.52it/s] 41%|####1     |71986/173481[18:00<27:36,61.28it/s] 42%|####1     |72633/173481[18:10<27:25,61.28it/s] 47%|####7     |82348/173481[21:00<25:35,59.36it/s] 48%|####7     |82971/173481[21:11<25:24,59.36it/s] 54%|#####3    |92940/173481[24:00<22:42,59.10it/s] 54%|#####3    |93603/173481[24:11<22:31,59.10it/s] 60%|#####9    |104068/173481[27:00<19:08,60.43it/s] 60%|######    |104739/173481[27:11<18:57,60.43it/s] 66%|######6   |114715/173481[30:00<16:23,59.78it/s] 67%|######6   |115398/173481[30:11<16:11,59.78it/s] 73%|#######2  |126426/173481[33:00<12:35,62.31it/s] 73%|#######3  |127177/173481[33:11<12:23,62.31it/s] 80%|#######9  |138262/173481[36:00<09:10,63.98it/s] 80%|########  |139083/173481[36:11<08:57,63.98it/s] 87%|########6 |150714/173481[39:00<05:42,66.48it/s] 87%|########7 |151490/173481[39:11<05:30,66.48it/s] 94%|#########3|162400/173481[42:00<02:48,65.69it/s] 94%|#########4|163238/173481[42:12<02:35,65.69it/s]100%|##########|173481/173481[44:42<00:00,64.68it/s]
[32m[0321 19:29:14 @base.py:257][0m Epoch 14 (global_step 2428734) finished, time:2682.13 sec.
[32m[0321 19:29:15 @saver.py:84][0m Model saved to train_log/fcn2_w_8_a_32_quant_ends_True/model-2428734.
[32m[0321 19:29:16 @saver.py:155][0m Model with minimum 'val-error-top1' saved.
  0%|          |0/18822[00:00<?,?it/s]100%|##########|18822/18822[02:38<00:00,118.51it/s]
13
[32m[0321 19:31:55 @monitor.py:363][0m QueueInput/queue_size: 0.73336
[32m[0321 19:31:55 @monitor.py:363][0m activation-summaries/last_linear/output-rms: 72.564
[32m[0321 19:31:55 @monitor.py:363][0m activation-summaries/output-rms: 0.051316
[32m[0321 19:31:55 @monitor.py:363][0m cross_entropy_loss: 0.90557
[32m[0321 19:31:55 @monitor.py:363][0m lr: 3.125e-05
[32m[0321 19:31:55 @monitor.py:363][0m param-summary/last_linear/W-rms: 0.58981
[32m[0321 19:31:55 @monitor.py:363][0m param-summary/last_linear/b-rms: 2.2583
[32m[0321 19:31:55 @monitor.py:363][0m param-summary/linear0/W-rms: 0.24986
[32m[0321 19:31:55 @monitor.py:363][0m param-summary/linear0/b-rms: 0.06095
[32m[0321 19:31:55 @monitor.py:363][0m param-summary/linear1/W-rms: 0.25815
[32m[0321 19:31:55 @monitor.py:363][0m param-summary/linear1/b-rms: 0.063622
[32m[0321 19:31:55 @monitor.py:363][0m param-summary/linear2/W-rms: 0.25929
[32m[0321 19:31:55 @monitor.py:363][0m param-summary/linear2/b-rms: 0.066024
[32m[0321 19:31:55 @monitor.py:363][0m param-summary/linear3/W-rms: 0.25451
[32m[0321 19:31:55 @monitor.py:363][0m param-summary/linear3/b-rms: 0.062469
[32m[0321 19:31:55 @monitor.py:363][0m train-error-top1: 0.25417
[32m[0321 19:31:55 @monitor.py:363][0m val-error-top1: 0.28951
[32m[0321 19:31:55 @monitor.py:363][0m val-utt-error: 0.030284
[32m[0321 19:31:55 @monitor.py:363][0m validation_cost: 1.0426
[32m[0321 19:31:55 @monitor.py:363][0m wd_cost: 0.0040522
[32m[0321 19:31:55 @group.py:42][0m Callbacks took 160.964 sec in total. InferenceRunner: 158.835sec
[32m[0321 19:31:55 @base.py:247][0m Start Epoch 15 ...
  0%|          |0/173481[00:00<?,?it/s] 12%|#1        |20196/173481[03:00<22:46,112.20it/s] 12%|#2        |21319/173481[03:10<22:36,112.20it/s] 20%|#9        |34264/173481[06:00<25:11,92.13it/s]  20%|##        |35001/173481[06:10<25:03,92.13it/s] 27%|##7       |46840/173481[09:00<26:33,79.47it/s] 27%|##7       |47568/173481[09:10<26:24,79.47it/s] 34%|###4      |59702/173481[12:00<25:12,75.25it/s] 35%|###4      |60524/173481[12:10<25:01,75.25it/s] 42%|####1     |72421/173481[15:00<23:06,72.87it/s] 42%|####2     |73137/173481[15:10<22:56,72.87it/s] 49%|####9     |85459/173481[18:00<20:11,72.65it/s] 50%|####9     |86238/173481[18:10<20:00,72.65it/s] 57%|#####6    |98297/173481[21:00<17:24,71.98it/s] 57%|#####7    |98989/173481[21:11<17:14,71.98it/s] 63%|######3   |109705/173481[24:00<15:46,67.40it/s] 64%|######3   |110430/173481[24:11<15:35,67.40it/s] 70%|#######   |121502/173481[27:00<13:02,66.46it/s] 70%|#######   |122226/173481[27:11<12:51,66.46it/s] 77%|#######6  |133159/173481[30:00<10:14,65.60it/s] 77%|#######7  |133941/173481[30:11<10:02,65.60it/s] 84%|########3 |145225/173481[33:00<07:06,66.30it/s] 84%|########4 |146040/173481[33:11<06:53,66.30it/s] 91%|######### |157817/173481[36:00<03:50,68.08it/s] 91%|#########1|158616/173481[36:11<03:38,68.08it/s] 98%|#########8|170439/173481[39:00<00:44,69.08it/s] 99%|#########8|171228/173481[39:12<00:32,69.08it/s]100%|##########|173481/173481[39:44<00:00,72.76it/s]
[32m[0321 20:11:40 @base.py:257][0m Epoch 15 (global_step 2602215) finished, time:2384.40 sec.
[32m[0321 20:11:40 @saver.py:84][0m Model saved to train_log/fcn2_w_8_a_32_quant_ends_True/model-2602215.
[32m[0321 20:11:42 @saver.py:155][0m Model with minimum 'val-error-top1' saved.
  0%|          |0/18822[00:00<?,?it/s]100%|##########|18822/18822[01:53<00:00,165.47it/s]
14
[32m[0321 20:13:36 @monitor.py:363][0m QueueInput/queue_size: 0.75218
[32m[0321 20:13:36 @monitor.py:363][0m activation-summaries/last_linear/output-rms: 75.944
[32m[0321 20:13:36 @monitor.py:363][0m activation-summaries/output-rms: 0.050656
[32m[0321 20:13:36 @monitor.py:363][0m cross_entropy_loss: 0.92826
[32m[0321 20:13:36 @monitor.py:363][0m lr: 3.125e-05
[32m[0321 20:13:36 @monitor.py:363][0m param-summary/last_linear/W-rms: 0.60874
[32m[0321 20:13:36 @monitor.py:363][0m param-summary/last_linear/b-rms: 2.2583
[32m[0321 20:13:36 @monitor.py:363][0m param-summary/linear0/W-rms: 0.2554
[32m[0321 20:13:36 @monitor.py:363][0m param-summary/linear0/b-rms: 0.06095
[32m[0321 20:13:36 @monitor.py:363][0m param-summary/linear1/W-rms: 0.25992
[32m[0321 20:13:36 @monitor.py:363][0m param-summary/linear1/b-rms: 0.063622
[32m[0321 20:13:36 @monitor.py:363][0m param-summary/linear2/W-rms: 0.26081
[32m[0321 20:13:36 @monitor.py:363][0m param-summary/linear2/b-rms: 0.066024
[32m[0321 20:13:36 @monitor.py:363][0m param-summary/linear3/W-rms: 0.25605
[32m[0321 20:13:36 @monitor.py:363][0m param-summary/linear3/b-rms: 0.062469
[32m[0321 20:13:36 @monitor.py:363][0m train-error-top1: 0.26515
[32m[0321 20:13:36 @monitor.py:363][0m val-error-top1: 0.28607
[32m[0321 20:13:36 @monitor.py:363][0m val-utt-error: 0.028052
[32m[0321 20:13:36 @monitor.py:363][0m validation_cost: 1.0301
[32m[0321 20:13:36 @monitor.py:363][0m wd_cost: 0.004211
[32m[0321 20:13:36 @group.py:42][0m Callbacks took 116.138 sec in total. InferenceRunner: 113.760sec
[32m[0321 20:13:36 @base.py:247][0m Start Epoch 16 ...
  0%|          |0/173481[00:00<?,?it/s] 11%|#1        |19816/173481[03:00<23:16,110.07it/s] 12%|#1        |20546/173481[03:10<23:09,110.07it/s] 19%|#8        |32362/173481[06:00<27:33,85.34it/s]  19%|#9        |33069/173481[06:10<27:25,85.34it/s] 26%|##6       |45138/173481[09:00<27:36,77.50it/s] 26%|##6       |45873/173481[09:10<27:26,77.50it/s] 33%|###3      |57694/173481[12:00<26:17,73.42it/s] 34%|###3      |58443/173481[12:10<26:06,73.42it/s] 41%|####      |70513/173481[15:00<23:44,72.30it/s] 41%|####1     |71235/173481[15:10<23:34,72.30it/s] 48%|####7     |82557/173481[18:00<21:48,69.50it/s] 48%|####8     |83325/173481[18:10<21:37,69.50it/s] 55%|#####4    |95313/173481[21:00<18:33,70.18it/s] 55%|#####5    |96105/173481[21:11<18:22,70.18it/s] 62%|######2   |108338/173481[24:00<15:14,71.25it/s] 63%|######2   |109143/173481[24:11<15:02,71.25it/s] 69%|######8   |119398/173481[27:00<13:39,65.98it/s] 69%|######9   |120082/173481[27:11<13:29,65.98it/s] 75%|#######5  |130432/173481[30:00<11:17,63.55it/s] 76%|#######5  |131301/173481[30:11<11:03,63.55it/s] 82%|########2 |142952/173481[33:00<07:39,66.42it/s] 83%|########2 |143769/173481[33:11<07:27,66.42it/s] 90%|########9 |155896/173481[36:00<04:14,69.05it/s] 90%|######### |156756/173481[36:11<04:02,69.05it/s] 97%|#########7|168694/173481[39:00<01:08,70.05it/s] 98%|#########7|169461/173481[39:12<00:57,70.05it/s]100%|##########|173481/173481[40:11<00:00,71.94it/s]
[32m[0321 20:53:47 @base.py:257][0m Epoch 16 (global_step 2775696) finished, time:2411.56 sec.
[32m[0321 20:53:48 @saver.py:84][0m Model saved to train_log/fcn2_w_8_a_32_quant_ends_True/model-2775696.
[32m[0321 20:53:49 @saver.py:155][0m Model with minimum 'val-error-top1' saved.
  0%|          |0/18822[00:00<?,?it/s]100%|##########|18822/18822[02:57<00:00,106.10it/s]
15
[32m[0321 20:56:47 @monitor.py:363][0m QueueInput/queue_size: 0.66458
[32m[0321 20:56:47 @monitor.py:363][0m activation-summaries/last_linear/output-rms: 79.648
[32m[0321 20:56:47 @monitor.py:363][0m activation-summaries/output-rms: 0.050221
[32m[0321 20:56:47 @monitor.py:363][0m cross_entropy_loss: 0.95509
[32m[0321 20:56:47 @monitor.py:363][0m lr: 3.125e-05
[32m[0321 20:56:47 @monitor.py:363][0m param-summary/last_linear/W-rms: 0.6273
[32m[0321 20:56:47 @monitor.py:363][0m param-summary/last_linear/b-rms: 2.2585
[32m[0321 20:56:47 @monitor.py:363][0m param-summary/linear0/W-rms: 0.26033
[32m[0321 20:56:47 @monitor.py:363][0m param-summary/linear0/b-rms: 0.06095
[32m[0321 20:56:47 @monitor.py:363][0m param-summary/linear1/W-rms: 0.26164
[32m[0321 20:56:47 @monitor.py:363][0m param-summary/linear1/b-rms: 0.063622
[32m[0321 20:56:47 @monitor.py:363][0m param-summary/linear2/W-rms: 0.26231
[32m[0321 20:56:47 @monitor.py:363][0m param-summary/linear2/b-rms: 0.066024
[32m[0321 20:56:47 @monitor.py:363][0m param-summary/linear3/W-rms: 0.25757
[32m[0321 20:56:47 @monitor.py:363][0m param-summary/linear3/b-rms: 0.06247
[32m[0321 20:56:47 @monitor.py:363][0m train-error-top1: 0.26777
[32m[0321 20:56:47 @monitor.py:363][0m val-error-top1: 0.28548
[32m[0321 20:56:47 @monitor.py:363][0m val-utt-error: 0.027149
[32m[0321 20:56:47 @monitor.py:363][0m validation_cost: 1.0294
[32m[0321 20:56:47 @monitor.py:363][0m wd_cost: 0.0043664
[32m[0321 20:56:47 @group.py:42][0m Callbacks took 179.474 sec in total. InferenceRunner: 177.406sec
[32m[0321 20:56:47 @base.py:247][0m Start Epoch 17 ...
  0%|          |0/173481[00:00<?,?it/s] 11%|#1        |19579/173481[03:00<23:34,108.77it/s] 12%|#1        |20667/173481[03:10<23:24,108.77it/s] 21%|##        |36430/173481[06:00<22:42,100.62it/s] 21%|##1       |37134/173481[06:10<22:35,100.62it/s] 28%|##8       |49003/173481[09:00<25:09,82.46it/s]  29%|##8       |49752/173481[09:10<25:00,82.46it/s] 36%|###5      |61639/173481[12:00<24:34,75.83it/s] 36%|###5      |62394/173481[12:10<24:24,75.83it/s] 43%|####2     |74161/173481[15:00<22:48,72.56it/s] 43%|####3     |74870/173481[15:10<22:39,72.56it/s] 50%|####9     |86367/173481[18:00<20:42,70.10it/s] 50%|#####     |87126/173481[18:10<20:31,70.10it/s] 57%|#####6    |98879/173481[21:00<17:48,69.81it/s] 57%|#####7    |99648/173481[21:11<17:37,69.81it/s] 64%|######3   |110455/173481[24:00<15:41,66.92it/s] 64%|######3   |110726/173481[24:11<15:37,66.92it/s] 71%|#######1  |123831/173481[27:00<11:45,70.42it/s] 72%|#######1  |124644/173481[27:11<11:33,70.42it/s] 78%|#######8  |135736/173481[30:00<09:13,68.21it/s] 79%|#######8  |136470/173481[30:11<09:02,68.21it/s] 85%|########5 |147667/173481[33:00<06:23,67.23it/s] 86%|########5 |148404/173481[33:11<06:13,67.23it/s] 92%|#########2|159622/173481[36:00<03:27,66.82it/s] 92%|#########2|160422/173481[36:11<03:15,66.82it/s] 99%|#########8|171523/173481[39:00<00:29,66.46it/s] 99%|#########9|172374/173481[39:12<00:16,66.46it/s]100%|##########|173481/173481[39:27<00:00,73.28it/s]
[32m[0321 21:36:14 @base.py:257][0m Epoch 17 (global_step 2949177) finished, time:2367.41 sec.
[32m[0321 21:36:15 @saver.py:84][0m Model saved to train_log/fcn2_w_8_a_32_quant_ends_True/model-2949177.
[32m[0321 21:36:16 @saver.py:155][0m Model with minimum 'val-error-top1' saved.
  0%|          |0/18822[00:00<?,?it/s]100%|##########|18822/18822[02:00<00:00,156.54it/s]
16
[32m[0321 21:38:16 @monitor.py:363][0m QueueInput/queue_size: 0.85625
[32m[0321 21:38:16 @monitor.py:363][0m activation-summaries/last_linear/output-rms: 83.136
[32m[0321 21:38:16 @monitor.py:363][0m activation-summaries/output-rms: 0.050998
[32m[0321 21:38:16 @monitor.py:363][0m cross_entropy_loss: 0.92735
[32m[0321 21:38:16 @monitor.py:363][0m lr: 1.5625e-05
[32m[0321 21:38:16 @monitor.py:363][0m param-summary/last_linear/W-rms: 0.63973
[32m[0321 21:38:16 @monitor.py:363][0m param-summary/last_linear/b-rms: 2.2586
[32m[0321 21:38:16 @monitor.py:363][0m param-summary/linear0/W-rms: 0.26324
[32m[0321 21:38:16 @monitor.py:363][0m param-summary/linear0/b-rms: 0.06095
[32m[0321 21:38:16 @monitor.py:363][0m param-summary/linear1/W-rms: 0.26256
[32m[0321 21:38:16 @monitor.py:363][0m param-summary/linear1/b-rms: 0.063622
[32m[0321 21:38:16 @monitor.py:363][0m param-summary/linear2/W-rms: 0.26302
[32m[0321 21:38:16 @monitor.py:363][0m param-summary/linear2/b-rms: 0.066024
[32m[0321 21:38:16 @monitor.py:363][0m param-summary/linear3/W-rms: 0.25832
[32m[0321 21:38:16 @monitor.py:363][0m param-summary/linear3/b-rms: 0.06247
[32m[0321 21:38:16 @monitor.py:363][0m train-error-top1: 0.25951
[32m[0321 21:38:16 @monitor.py:363][0m val-error-top1: 0.28088
[32m[0321 21:38:16 @monitor.py:363][0m val-utt-error: 0.026618
[32m[0321 21:38:16 @monitor.py:363][0m validation_cost: 1.0084
[32m[0321 21:38:16 @monitor.py:363][0m wd_cost: 0.00089315
[32m[0321 21:38:16 @group.py:42][0m Callbacks took 122.354 sec in total. InferenceRunner: 120.251sec
[32m[0321 21:38:16 @base.py:247][0m Start Epoch 18 ...
  0%|          |0/173481[00:00<?,?it/s] 11%|#         |19078/173481[03:00<24:16,105.99it/s] 12%|#1        |20143/173481[03:10<24:06,105.99it/s] 19%|#9        |33641/173481[06:00<25:23,91.76it/s]  20%|#9        |34390/173481[06:10<25:15,91.76it/s] 27%|##6       |46564/173481[09:00<26:15,80.56it/s] 27%|##7       |47292/173481[09:10<26:06,80.56it/s] 34%|###4      |59142/173481[12:00<25:27,74.84it/s] 35%|###4      |59881/173481[12:10<25:17,74.84it/s] 41%|####1     |71752/173481[15:00<23:25,72.36it/s] 42%|####1     |72507/173481[15:10<23:15,72.36it/s] 49%|####8     |84335/173481[18:00<20:53,71.11it/s] 49%|####9     |85094/173481[18:11<20:42,71.11it/s] 55%|#####4    |95194/173481[21:00<20:00,65.23it/s] 55%|#####5    |96270/173481[21:11<19:43,65.23it/s] 63%|######2   |108918/173481[24:00<15:18,70.31it/s] 63%|######3   |109713/173481[24:11<15:06,70.31it/s] 70%|#######   |121642/173481[27:00<12:15,70.50it/s] 71%|#######   |122421/173481[27:11<12:04,70.50it/s] 77%|#######7  |134302/173481[30:00<09:16,70.41it/s] 78%|#######7  |135081/173481[30:11<09:05,70.41it/s] 85%|########4 |146812/173481[33:00<06:21,69.93it/s] 85%|########5 |147627/173481[33:11<06:09,69.93it/s] 92%|#########1|159203/173481[36:00<03:25,69.38it/s] 92%|#########2|159993/173481[36:11<03:14,69.38it/s] 99%|#########8|171615/173481[39:00<00:26,69.17it/s] 99%|#########9|172449/173481[39:12<00:14,69.17it/s]100%|##########|173481/173481[39:27<00:00,73.29it/s]
[32m[0321 22:17:44 @base.py:257][0m Epoch 18 (global_step 3122658) finished, time:2367.17 sec.
[32m[0321 22:17:44 @saver.py:84][0m Model saved to train_log/fcn2_w_8_a_32_quant_ends_True/model-3122658.
[32m[0321 22:17:46 @saver.py:155][0m Model with minimum 'val-error-top1' saved.
  0%|          |0/18822[00:00<?,?it/s]100%|##########|18822/18822[01:49<00:00,171.40it/s]
17
[32m[0321 22:19:36 @monitor.py:363][0m QueueInput/queue_size: 0.70881
[32m[0321 22:19:36 @monitor.py:363][0m activation-summaries/last_linear/output-rms: 85.046
[32m[0321 22:19:36 @monitor.py:363][0m activation-summaries/output-rms: 0.051133
[32m[0321 22:19:36 @monitor.py:363][0m cross_entropy_loss: 0.87227
[32m[0321 22:19:36 @monitor.py:363][0m lr: 1.5625e-05
[32m[0321 22:19:36 @monitor.py:363][0m param-summary/last_linear/W-rms: 0.64944
[32m[0321 22:19:36 @monitor.py:363][0m param-summary/last_linear/b-rms: 2.2586
[32m[0321 22:19:36 @monitor.py:363][0m param-summary/linear0/W-rms: 0.26551
[32m[0321 22:19:36 @monitor.py:363][0m param-summary/linear0/b-rms: 0.06095
[32m[0321 22:19:36 @monitor.py:363][0m param-summary/linear1/W-rms: 0.2631
[32m[0321 22:19:36 @monitor.py:363][0m param-summary/linear1/b-rms: 0.063622
[32m[0321 22:19:36 @monitor.py:363][0m param-summary/linear2/W-rms: 0.26344
[32m[0321 22:19:36 @monitor.py:363][0m param-summary/linear2/b-rms: 0.066024
[32m[0321 22:19:36 @monitor.py:363][0m param-summary/linear3/W-rms: 0.25874
[32m[0321 22:19:36 @monitor.py:363][0m param-summary/linear3/b-rms: 0.06247
[32m[0321 22:19:36 @monitor.py:363][0m train-error-top1: 0.24959
[32m[0321 22:19:36 @monitor.py:363][0m val-error-top1: 0.27955
[32m[0321 22:19:36 @monitor.py:363][0m val-utt-error: 0.026033
[32m[0321 22:19:36 @monitor.py:363][0m validation_cost: 1.0029
[32m[0321 22:19:36 @monitor.py:363][0m wd_cost: 0.0009085
[32m[0321 22:19:36 @group.py:42][0m Callbacks took 112.142 sec in total. InferenceRunner: 109.827sec
[32m[0321 22:19:36 @base.py:247][0m Start Epoch 19 ...
  0%|          |0/173481[00:00<?,?it/s]  7%|7         |12979/173481[03:00<37:05,72.11it/s]  8%|7         |13714/173481[03:10<36:55,72.11it/s] 15%|#4        |25596/173481[06:00<34:40,71.08it/s] 15%|#5        |26281/173481[06:10<34:30,71.08it/s] 22%|##1       |37986/173481[09:00<32:17,69.94it/s] 22%|##2       |38715/173481[09:10<32:06,69.94it/s] 29%|##9       |50395/173481[12:00<29:32,69.43it/s] 29%|##9       |51138/173481[12:10<29:22,69.43it/s] 36%|###5      |62363/173481[15:00<27:15,67.92it/s] 36%|###6      |63072/173481[15:10<27:05,67.92it/s] 43%|####2     |74099/173481[18:00<24:53,66.53it/s] 43%|####3     |75234/173481[18:11<24:36,66.53it/s] 50%|####9     |86221/173481[21:00<21:43,66.93it/s] 50%|#####     |86916/173481[21:11<21:33,66.93it/s] 57%|#####6    |98059/173481[24:00<18:56,66.34it/s] 57%|#####6    |98796/173481[24:11<18:45,66.34it/s] 63%|######3   |109645/173481[27:00<16:17,65.33it/s] 64%|######3   |110377/173481[27:11<16:05,65.33it/s] 70%|#######   |121621/173481[30:00<13:06,65.91it/s] 71%|#######   |122335/173481[30:11<12:56,65.91it/s] 77%|#######7  |133633/173481[33:00<10:00,66.31it/s] 77%|#######7  |134436/173481[33:11<09:48,66.31it/s] 84%|########4 |145855/173481[36:00<06:51,67.09it/s] 85%|########4 |146628/173481[36:11<06:40,67.09it/s] 91%|#########1|158239/173481[39:00<03:44,67.93it/s] 92%|#########1|159006/173481[39:12<03:33,67.93it/s] 98%|#########8|170775/173481[42:00<00:39,68.77it/s] 99%|#########8|171648/173481[42:12<00:26,68.77it/s]100%|##########|173481/173481[42:38<00:00,67.80it/s]
[32m[0321 23:02:15 @base.py:257][0m Epoch 19 (global_step 3296139) finished, time:2558.90 sec.
[32m[0321 23:02:15 @saver.py:84][0m Model saved to train_log/fcn2_w_8_a_32_quant_ends_True/model-3296139.
[32m[0321 23:02:16 @saver.py:155][0m Model with minimum 'val-error-top1' saved.
  0%|          |0/18822[00:00<?,?it/s]100%|##########|18822/18822[02:08<00:00,146.58it/s]
18
[32m[0321 23:04:25 @monitor.py:363][0m QueueInput/queue_size: 0.51111
[32m[0321 23:04:25 @monitor.py:363][0m activation-summaries/last_linear/output-rms: 86.783
[32m[0321 23:04:25 @monitor.py:363][0m activation-summaries/output-rms: 0.050796
[32m[0321 23:04:25 @monitor.py:363][0m cross_entropy_loss: 0.94211
[32m[0321 23:04:25 @monitor.py:363][0m lr: 1.5625e-05
[32m[0321 23:04:25 @monitor.py:363][0m param-summary/last_linear/W-rms: 0.65928
[32m[0321 23:04:25 @monitor.py:363][0m param-summary/last_linear/b-rms: 2.2587
[32m[0321 23:04:25 @monitor.py:363][0m param-summary/linear0/W-rms: 0.26768
[32m[0321 23:04:25 @monitor.py:363][0m param-summary/linear0/b-rms: 0.06095
[32m[0321 23:04:25 @monitor.py:363][0m param-summary/linear1/W-rms: 0.26367
[32m[0321 23:04:25 @monitor.py:363][0m param-summary/linear1/b-rms: 0.063622
[32m[0321 23:04:25 @monitor.py:363][0m param-summary/linear2/W-rms: 0.26386
[32m[0321 23:04:25 @monitor.py:363][0m param-summary/linear2/b-rms: 0.066024
[32m[0321 23:04:25 @monitor.py:363][0m param-summary/linear3/W-rms: 0.25917
[32m[0321 23:04:25 @monitor.py:363][0m param-summary/linear3/b-rms: 0.06247
[32m[0321 23:04:25 @monitor.py:363][0m train-error-top1: 0.25988
[32m[0321 23:04:25 @monitor.py:363][0m val-error-top1: 0.27804
[32m[0321 23:04:25 @monitor.py:363][0m val-utt-error: 0.026724
[32m[0321 23:04:25 @monitor.py:363][0m validation_cost: 0.99938
[32m[0321 23:04:25 @monitor.py:363][0m wd_cost: 0.00092404
[32m[0321 23:04:25 @group.py:42][0m Callbacks took 129.905 sec in total. InferenceRunner: 128.423sec
[32m[0321 23:04:25 @base.py:247][0m Start Epoch 20 ...
  0%|          |0/173481[00:00<?,?it/s] 11%|#1        |19146/173481[03:00<24:10,106.37it/s] 12%|#1        |20212/173481[03:10<24:00,106.37it/s] 19%|#8        |32566/173481[06:00<26:47,87.66it/s]  19%|#9        |33257/173481[06:10<26:39,87.66it/s] 26%|##5       |44878/173481[09:00<27:53,76.83it/s] 26%|##6       |45587/173481[09:10<27:44,76.83it/s] 32%|###2      |55522/173481[12:00<29:25,66.82it/s] 32%|###2      |56234/173481[12:10<29:14,66.82it/s] 39%|###8      |67529/173481[15:00<26:26,66.76it/s] 39%|###9      |68244/173481[15:10<26:16,66.76it/s] 46%|####5     |79019/173481[18:00<24:07,65.26it/s] 46%|####5     |79692/173481[18:11<23:57,65.26it/s]slurmstepd: *** JOB 81841 ON sls-sm-6 CANCELLED AT 2018-03-21T23:24:26 ***
srun: got SIGCONT
srun: forcing job termination
slurmstepd: *** STEP 81841.0 ON sls-sm-6 CANCELLED AT 2018-03-21T23:24:26 ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
