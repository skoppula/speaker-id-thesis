{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorpack as tp\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from helpers.rsr_run import Model\n",
    "from helpers.rsr_run import create_dataflow\n",
    "from helpers.rsr_run import net_fn_map\n",
    "from helpers.rsr2015 import *\n",
    "from tensorpack import *\n",
    "from tensorpack.tfutils.varmanip import *\n",
    "from tensorpack.utils.gpu import get_nr_gpu\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def fuse_bn_params(kernel, bias, beta, gamma, mean_ema, var_ema):\n",
    "    kernel,bias = kernel.astype(np.float64), bias.astype(np.float64)\n",
    "    beta,gamma = beta.astype(np.float64), gamma.astype(np.float64)\n",
    "    mean_ema,var_ema = mean_ema.astype(np.float64), var_ema.astype(np.float64)\n",
    "    scale = gamma/np.sqrt(var_ema+1e-5)\n",
    "    new_kernel = kernel*scale\n",
    "    new_bias = beta-scale*(bias+mean_ema)\n",
    "    return new_kernel.astype(np.float32), new_bias.astype(np.float32)\n",
    "\n",
    "def reorder(names):\n",
    "    names = sorted(names)\n",
    "    for i, name in enumerate(names):\n",
    "        if '/depthwise_weights' in name:\n",
    "            var = names.pop(i)\n",
    "            names.insert(0, var)\n",
    "            break\n",
    "    for i, name in enumerate(names):\n",
    "        if '/biases' in name:\n",
    "            var = names.pop(i)\n",
    "            names.insert(1, var)\n",
    "            break\n",
    "    return names\n",
    "\n",
    "def fuse_bn_layer(var_dict, layer_var_names):\n",
    "    layer_vars = reorder(layer_var_names)\n",
    "    print(layer_vars)\n",
    "    layer_vars = [var_dict[var] for var in layer_vars]\n",
    "    \n",
    "    if len(layer_vars) == 6:\n",
    "        return fuse_bn_params(*layer_vars)\n",
    "    else: assert False\n",
    "\n",
    "def group_bn_vars(var_dict):\n",
    "    all_bn_vars = set([var for var in var_dict if '/bn/' in var and 'Adam' not in var])\n",
    "    base_scopes_for_bn = set([var.split('/bn/')[0] for var in all_bn_vars])\n",
    "    groups = {scope:[var for var in all_bn_vars if scope in var] for scope in base_scopes_for_bn}\n",
    "    for scope in groups:\n",
    "        for var in var_dict:\n",
    "            if 'Adam' in var or '/bn/' in var: continue\n",
    "            if scope in var and (var.endswith('/W') or var.endswith('/b') or var.endswith('/depthwise_weights') or var.endswith('/biases')):\n",
    "                groups[scope].insert(0, var)\n",
    "    return groups\n",
    "\n",
    "def fuse_bn_layers(var_dict):\n",
    "    groups = group_bn_vars(var_dict)\n",
    "    print(groups)\n",
    "    new_var_dict = {}\n",
    "    new_groups = {}\n",
    "    replaced_vars = set()\n",
    "    for layer in groups:\n",
    "        new_w, new_b = fuse_bn_layer(var_dict, groups[layer])\n",
    "        for var_name in groups[layer]:\n",
    "            if var_name.endswith('/W') or var_name.endswith('/depthwise_weights'):\n",
    "                new_var_dict[var_name] = new_w\n",
    "                replaced_vars.add(var_name)\n",
    "            elif var_name.endswith('/b') or var_name.endswith('/biases'):\n",
    "                new_var_dict[var_name] = new_b\n",
    "                replaced_vars.add(var_name)\n",
    "        print(\"processing layer\", layer, new_w.shape, new_b.shape)\n",
    "    for var in var_dict:\n",
    "        if '/bn/' in var or var in replaced_vars or 'Adam' in var: continue\n",
    "        new_var_dict[var] = var_dict[var]\n",
    "    return new_var_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'fcn2'\n",
    "# ckpt_path = '/data/sls/u/meng/skanda/home/thesis/manfxpt/models/sentfiltNone_{}_bnTrue_regFalse_noLRSchedule/checkpoint'.format(model_name)\n",
    "ckpt_path = '/data/sls/u/meng/skanda/home/thesis/manfxpt/models/sentfiltNone_{}_bnTrue_regTrue_noLRSchedule/checkpoint'.format(model_name)\n",
    "datadir='/data/sls/scratch/skoppula/kaldi-rsr/numpy/'\n",
    "spkmap='/data/sls/scratch/skoppula/backup-exps/rsr-experiments/create_rsr_data_cache/generator_full_dataset/spk_mappings.pickle'\n",
    "context=50\n",
    "outdir=os.path.join('no_bn_models', '_'.join([str(x) for x in [model_name]]))\n",
    "cachedir='/data/sls/scratch/skoppula/backup-exps/rsr-experiments/create_rsr_data_cache/trn_cache/context_50frms/'\n",
    "n_spks = get_n_spks(spkmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird numerical issues if trying to run one step of re-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(n_spks, net_fn_map[model_name], bn=False, reg=True, n_context=context, qtype=None)\n",
    "var_dict = load_chkpt_vars(ckpt_path)\n",
    "new_var_dict = fuse_bn_layers(var_dict)\n",
    "\n",
    "with TowerContext('', is_training=False):\n",
    "    input = PlaceholderInput()\n",
    "    input.setup(model.get_inputs_desc())\n",
    "    model.build_graph(*input.get_input_tensors())\n",
    "\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "    init = sessinit.DictRestore(new_var_dict)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    init.init(sess)\n",
    "    \n",
    "    ms = ModelSaver(checkpoint_dir=outdir)\n",
    "    ms._setup_graph()\n",
    "    time = datetime.now().strftime('%m%d-%H%M%S')\n",
    "    ms.saver.export_meta_graph(os.path.join(ms.checkpoint_dir, 'graph-{}.meta'.format(time)), collection_list=tf.get_default_graph().get_all_collection_keys())\n",
    "    ms.saver.save(sess, ms.path, global_step=0, write_meta_graph=False)\n",
    "\n",
    "    np.savez_compressed(os.path.join(outdir, 'params.npz'), **new_var_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataflow, n_batches_val = create_dataflow('val', None, datadir, spkmap, None, context)\n",
    "val_generator = val_dataflow.get_data()\n",
    "var_dict2 = load_chkpt_vars(os.path.join(outdir,'checkpoint'))\n",
    "\n",
    "print(outdir)\n",
    "config = PredictConfig(\n",
    "        model=model,\n",
    "        session_init=SaverRestore(os.path.join(outdir, 'checkpoint')),\n",
    "        # session_init=DictRestore(var_dict2),\n",
    "        input_names=['input', 'label'],\n",
    "        output_names=['utt-wrong', 'train-error-top1', 'linear0/Reshape', 'linear0/BiasAdd']\n",
    ")\n",
    "predictor = OfflinePredictor(config)\n",
    "\n",
    "rc = tp.utils.stats.RatioCounter()\n",
    "linear0_out = None\n",
    "ratios2 = []\n",
    "for i in range(n_batches_val):\n",
    "    x,y = next(val_generator)\n",
    "    utt_wrong,te,inp,out = predictor([x,y])\n",
    "    rc.feed(utt_wrong,1)\n",
    "    if i % 100 == 0:\n",
    "        print(\"On\",i,\"of\",n_batches_val, \"error:\", rc.ratio)\n",
    "        ratios2.append(rc.ratio)\n",
    "    if i == 200: break\n",
    "rc.ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
