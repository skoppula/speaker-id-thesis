{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorpack as tp\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from helpers.rsr_run import Model\n",
    "from helpers.rsr_run import create_dataflow\n",
    "from helpers.rsr_run import net_fn_map\n",
    "from helpers.rsr2015 import *\n",
    "from tensorpack import *\n",
    "from tensorpack.tfutils.varmanip import *\n",
    "from tensorpack.utils.gpu import get_nr_gpu\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def fuse_bn_params(kernel, bias, beta, gamma, mean_ema, var_ema):\n",
    "    kernel,bias = kernel.astype(np.float64), bias.astype(np.float64)\n",
    "    beta,gamma = beta.astype(np.float64), gamma.astype(np.float64)\n",
    "    mean_ema,var_ema = mean_ema.astype(np.float64), var_ema.astype(np.float64)\n",
    "    scale = gamma/np.sqrt(var_ema+1e-5)\n",
    "    new_kernel = kernel*scale\n",
    "    new_bias = beta-scale*(bias+mean_ema)\n",
    "    return new_kernel.astype(np.float32), new_bias.astype(np.float32)\n",
    "\n",
    "def reorder(names):\n",
    "    names = sorted(names)\n",
    "    for i, name in enumerate(names):\n",
    "        if '/depthwise_weights' in name:\n",
    "            var = names.pop(i)\n",
    "            names.insert(0, var)\n",
    "            break\n",
    "    for i, name in enumerate(names):\n",
    "        if '/biases' in name:\n",
    "            var = names.pop(i)\n",
    "            names.insert(1, var)\n",
    "            break\n",
    "    return names\n",
    "\n",
    "def fuse_bn_layer(var_dict, layer_var_names):\n",
    "    layer_vars = reorder(layer_var_names)\n",
    "    print(layer_vars)\n",
    "    layer_vars = [var_dict[var] for var in layer_vars]\n",
    "    \n",
    "    if len(layer_vars) == 6:\n",
    "        return fuse_bn_params(*layer_vars)\n",
    "    else: assert False\n",
    "\n",
    "def group_bn_vars(var_dict):\n",
    "    all_bn_vars = set([var for var in var_dict if '/bn/' in var and 'Adam' not in var])\n",
    "    base_scopes_for_bn = set([var.split('/bn/')[0] for var in all_bn_vars])\n",
    "    groups = {scope:[var for var in all_bn_vars if scope in var] for scope in base_scopes_for_bn}\n",
    "    for scope in groups:\n",
    "        for var in var_dict:\n",
    "            if 'Adam' in var or '/bn/' in var: continue\n",
    "            if scope in var and (var.endswith('/W') or var.endswith('/b') or var.endswith('/depthwise_weights') or var.endswith('/biases')):\n",
    "                groups[scope].insert(0, var)\n",
    "    return groups\n",
    "\n",
    "def fuse_bn_layers(var_dict):\n",
    "    groups = group_bn_vars(var_dict)\n",
    "    print(groups)\n",
    "    new_var_dict = {}\n",
    "    new_groups = {}\n",
    "    replaced_vars = set()\n",
    "    for layer in groups:\n",
    "        new_w, new_b = fuse_bn_layer(var_dict, groups[layer])\n",
    "        for var_name in groups[layer]:\n",
    "            if var_name.endswith('/W') or var_name.endswith('/depthwise_weights'):\n",
    "                new_var_dict[var_name] = new_w\n",
    "                replaced_vars.add(var_name)\n",
    "            elif var_name.endswith('/b') or var_name.endswith('/biases'):\n",
    "                new_var_dict[var_name] = new_b\n",
    "                replaced_vars.add(var_name)\n",
    "        print(\"processing layer\", layer, new_w.shape, new_b.shape)\n",
    "    for var in var_dict:\n",
    "        if '/bn/' in var or var in replaced_vars or 'Adam' in var: continue\n",
    "        new_var_dict[var] = var_dict[var]\n",
    "    return new_var_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'fcn2'\n",
    "# ckpt_path = '/data/sls/u/meng/skanda/home/thesis/manfxpt/models/sentfiltNone_{}_bnTrue_regFalse_noLRSchedule/checkpoint'.format(model_name)\n",
    "ckpt_path = '/data/sls/u/meng/skanda/home/thesis/manfxpt/models/sentfiltNone_{}_bnTrue_regTrue_noLRSchedule/checkpoint'.format(model_name)\n",
    "datadir='/data/sls/scratch/skoppula/kaldi-rsr/numpy/'\n",
    "spkmap='/data/sls/scratch/skoppula/backup-exps/rsr-experiments/create_rsr_data_cache/generator_full_dataset/spk_mappings.pickle'\n",
    "context=50\n",
    "outdir=os.path.join('no_bn_models', '_'.join([str(x) for x in [model_name]]))\n",
    "cachedir='/data/sls/scratch/skoppula/backup-exps/rsr-experiments/create_rsr_data_cache/trn_cache/context_50frms/'\n",
    "n_spks = get_n_spks(spkmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird numerical issues if trying to run one step of re-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-39d3bd979c3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_spks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvar_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_chkpt_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnew_var_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuse_bn_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mTowerContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/sls/u/meng/skanda/home/thesis/manfxpt/helpers/rsr_run.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_spks, net_fn, bn, reg, n_context, qtype, w_bits, a_bits, overflow_rate)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_spks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_bits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_bits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverflow_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_spks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_spks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "model = Model(n_spks, net_fn_map[model_name], bn=False, reg=True, n_context=context, qtype=None)\n",
    "var_dict = load_chkpt_vars(ckpt_path)\n",
    "new_var_dict = fuse_bn_layers(var_dict)\n",
    "\n",
    "with TowerContext('', is_training=False):\n",
    "    input = PlaceholderInput()\n",
    "    input.setup(model.get_inputs_desc())\n",
    "    model.build_graph(*input.get_input_tensors())\n",
    "\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "    init = sessinit.DictRestore(new_var_dict)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    init.init(sess)\n",
    "    \n",
    "    ms = ModelSaver(checkpoint_dir=outdir)\n",
    "    ms._setup_graph()\n",
    "    time = datetime.now().strftime('%m%d-%H%M%S')\n",
    "    ms.saver.export_meta_graph(os.path.join(ms.checkpoint_dir, 'graph-{}.meta'.format(time)), collection_list=tf.get_default_graph().get_all_collection_keys())\n",
    "    ms.saver.save(sess, ms.path, global_step=0, write_meta_graph=False)\n",
    "\n",
    "    np.savez_compressed(os.path.join(outdir, 'params.npz'), **new_var_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('whole utterance size', 75290)\n",
      "('val', False, 75288)\n",
      "no_bn_models/fcn2\n",
      "('Adding activation tensors to summary:', [<tf.Tensor 'linear0/output:0' shape=(?, 504) dtype=float32>, <tf.Tensor 'linear1/output:0' shape=(?, 504) dtype=float32>, <tf.Tensor 'linear2/output:0' shape=(?, 504) dtype=float32>, <tf.Tensor 'linear3/output:0' shape=(?, 504) dtype=float32>, <tf.Tensor 'last_linear/output:0' shape=(?, 255) dtype=float32>, <tf.Tensor 'output:0' shape=(?, 255) dtype=float32>])\n",
      "\u001b[32m[0228 21:50:34 @rsr_run.py:126]\u001b[0m Parameter count: {'mults': 0, 'weights': 0}\n",
      "\u001b[32m[0228 21:50:34 @collection.py:134]\u001b[0m New collections created in : tf.GraphKeys.GLOBAL_STEP\n",
      "\u001b[32m[0228 21:50:34 @sessinit.py:116]\u001b[0m Restoring checkpoint from no_bn_models/fcn2/model-0 ...\n",
      "INFO:tensorflow:Restoring parameters from no_bn_models/fcn2/model-0\n",
      "0\n",
      "\u001b[32m[0228 21:50:34 @develop.py:85]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [Deprecated] Calling a predictor with one datapoint will be deprecated after 01 Mar. Call it with positional arguments instead!\n",
      "('On', 0, 'of', 75288, 'error:', array([0.]))\n",
      "('On', 100, 'of', 75288, 'error:', array([0.02970297]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.02970297])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataflow, n_batches_val = create_dataflow('val', None, datadir, spkmap, None, context)\n",
    "val_generator = val_dataflow.get_data()\n",
    "var_dict2 = load_chkpt_vars(os.path.join(outdir,'checkpoint'))\n",
    "\n",
    "print(outdir)\n",
    "config = PredictConfig(\n",
    "        model=model,\n",
    "        session_init=SaverRestore(os.path.join(outdir, 'checkpoint')),\n",
    "        # session_init=DictRestore(var_dict2),\n",
    "        input_names=['input', 'label'],\n",
    "        output_names=['utt-wrong', 'train-error-top1', 'linear0/Reshape', 'linear0/BiasAdd']\n",
    ")\n",
    "predictor = OfflinePredictor(config)\n",
    "\n",
    "rc = tp.utils.stats.RatioCounter()\n",
    "linear0_out = None\n",
    "ratios2 = []\n",
    "for i in range(n_batches_val):\n",
    "    x,y = next(val_generator)\n",
    "    utt_wrong,te,inp,out = predictor([x,y])\n",
    "    rc.feed(utt_wrong,1)\n",
    "    if i % 100 == 0:\n",
    "        print(\"On\",i,\"of\",n_batches_val, \"error:\", rc.ratio)\n",
    "        ratios2.append(rc.ratio)\n",
    "    if i == 200: break\n",
    "rc.ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
